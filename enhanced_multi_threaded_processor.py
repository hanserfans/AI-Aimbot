#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºçš„å¤šçº¿ç¨‹AIå¤„ç†å™¨
é›†æˆå¸§æ—¶é—´é¡ºåºç®¡ç†ï¼Œç¡®ä¿å¤„ç†æœ€æ–°å¸§
"""

import numpy as np
import torch
import onnxruntime as ort
import time
import threading
import queue
import heapq
from typing import Optional, Dict, List, Tuple, Any
import psutil
import pandas as pd
from utils.general import non_max_suppression, xyxy2xywh
import cv2

class FrameOrderingManager:
    """å¸§æ—¶é—´é¡ºåºç®¡ç†å™¨ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
    
    def __init__(self, max_frame_age: float = 0.05, buffer_size: int = 10):
        self.max_frame_age = max_frame_age
        self.buffer_size = buffer_size
        self.frame_heap = []  # å­˜å‚¨ (-timestamp, frame_id, frame_data)
        self.heap_lock = threading.Lock()
        self.frame_counter = 0
        self.counter_lock = threading.Lock()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'frames_received': 0,
            'frames_processed': 0,
            'frames_discarded_old': 0,
            'frames_discarded_overflow': 0,
            'avg_frame_age': 0.0
        }
    
    def add_frame(self, frame_data: Dict) -> bool:
        """æ·»åŠ å¸§åˆ°æœ‰åºç¼“å†²åŒº"""
        current_time = time.time()
        frame_timestamp = frame_data.get('timestamp', current_time)
        
        # æ£€æŸ¥å¸§æ˜¯å¦è¿‡æ—¶
        frame_age = current_time - frame_timestamp
        if frame_age > self.max_frame_age:
            self.stats['frames_discarded_old'] += 1
            return False
        
        with self.counter_lock:
            frame_id = self.frame_counter
            self.frame_counter += 1
        
        with self.heap_lock:
            # æ£€æŸ¥ç¼“å†²åŒºæ˜¯å¦å·²æ»¡
            if len(self.frame_heap) >= self.buffer_size:
                if self.frame_heap:
                    heapq.heappop(self.frame_heap)
                    self.stats['frames_discarded_overflow'] += 1
            
            # æ·»åŠ æ–°å¸§ï¼ˆä½¿ç”¨è´Ÿæ—¶é—´æˆ³ä½¿æœ€æ–°å¸§åœ¨å †é¡¶ï¼‰
            heapq.heappush(self.frame_heap, (-frame_timestamp, frame_id, frame_data))
            self.stats['frames_received'] += 1
        
        return True
    
    def get_latest_frame(self) -> Optional[Dict]:
        """è·å–æœ€æ–°çš„å¸§"""
        with self.heap_lock:
            if not self.frame_heap:
                return None
            
            # è·å–æœ€æ–°å¸§ï¼ˆå †é¡¶ï¼‰
            neg_timestamp, frame_id, frame_data = heapq.heappop(self.frame_heap)
            timestamp = -neg_timestamp
            
            # æ£€æŸ¥å¸§æ˜¯å¦ä»ç„¶æœ‰æ•ˆ
            current_time = time.time()
            frame_age = current_time - timestamp
            
            if frame_age > self.max_frame_age:
                self.stats['frames_discarded_old'] += 1
                return self.get_latest_frame()  # é€’å½’è·å–ä¸‹ä¸€ä¸ªå¸§
            
            self.stats['frames_processed'] += 1
            self.stats['avg_frame_age'] = (
                self.stats['avg_frame_age'] * (self.stats['frames_processed'] - 1) + frame_age
            ) / self.stats['frames_processed']
            
            # æ·»åŠ å¤„ç†æ—¶é—´ä¿¡æ¯
            frame_data['processing_timestamp'] = current_time
            frame_data['frame_age'] = frame_age
            frame_data['frame_id'] = frame_id
            
            return frame_data
    
    def get_batch_frames(self, batch_size: int) -> List[Dict]:
        """è·å–ä¸€æ‰¹æœ€æ–°å¸§"""
        frames = []
        for _ in range(batch_size):
            frame = self.get_latest_frame()
            if frame is None:
                break
            frames.append(frame)
        return frames
    
    def clear_old_frames(self):
        """æ¸…ç†è¿‡æ—¶å¸§"""
        current_time = time.time()
        
        with self.heap_lock:
            valid_frames = []
            
            while self.frame_heap:
                neg_timestamp, frame_id, frame_data = heapq.heappop(self.frame_heap)
                timestamp = -neg_timestamp
                frame_age = current_time - timestamp
                
                if frame_age <= self.max_frame_age:
                    valid_frames.append((neg_timestamp, frame_id, frame_data))
                else:
                    self.stats['frames_discarded_old'] += 1
            
            # é‡å»ºå †
            self.frame_heap = valid_frames
            heapq.heapify(self.frame_heap)
    
    def get_buffer_size(self) -> int:
        """è·å–å½“å‰ç¼“å†²åŒºå¤§å°"""
        with self.heap_lock:
            return len(self.frame_heap)


class EnhancedMultiThreadedAIProcessor:
    """å¢å¼ºçš„å¤šçº¿ç¨‹AIå¤„ç†å™¨ï¼Œé›†æˆå¸§æ—¶é—´é¡ºåºç®¡ç†"""
    
    def __init__(self,
                 model_path: str,
                 num_inference_threads: int = None,
                 num_postprocess_threads: int = None,
                 batch_size: int = 4,
                 enable_gpu_inference: bool = True,
                 max_frame_age: float = 0.05):
        """
        åˆå§‹åŒ–å¢å¼ºå¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿ
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            num_inference_threads: æ¨ç†çº¿ç¨‹æ•°
            num_postprocess_threads: åå¤„ç†çº¿ç¨‹æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°
            enable_gpu_inference: å¯ç”¨GPUæ¨ç†
            max_frame_age: æœ€å¤§å¸§å¹´é¾„ï¼ˆç§’ï¼‰
        """
        self.model_path = model_path
        self.batch_size = batch_size
        self.enable_gpu_inference = enable_gpu_inference
        self.max_frame_age = max_frame_age
        
        # è‡ªåŠ¨æ£€æµ‹æœ€ä¼˜çº¿ç¨‹æ•°
        cpu_count = psutil.cpu_count(logical=True)
        self.num_inference_threads = num_inference_threads or min(4, max(2, cpu_count // 4))
        self.num_postprocess_threads = num_postprocess_threads or min(8, max(4, cpu_count // 2))
        
        print(f"[INFO] ğŸš€ å¢å¼ºå¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿåˆå§‹åŒ–")
        print(f"   â€¢ æ¨ç†çº¿ç¨‹æ•°: {self.num_inference_threads}")
        print(f"   â€¢ åå¤„ç†çº¿ç¨‹æ•°: {self.num_postprocess_threads}")
        print(f"   â€¢ æ‰¹å¤„ç†å¤§å°: {batch_size}")
        print(f"   â€¢ GPUæ¨ç†: {enable_gpu_inference}")
        print(f"   â€¢ æœ€å¤§å¸§å¹´é¾„: {max_frame_age*1000:.1f}ms")
        
        # å¸§æ—¶é—´é¡ºåºç®¡ç†å™¨
        self.frame_manager = FrameOrderingManager(
            max_frame_age=max_frame_age,
            buffer_size=batch_size * 3  # ç¼“å†²åŒºå¤§å°ä¸ºæ‰¹å¤„ç†å¤§å°çš„3å€
        )
        
        # åˆå§‹åŒ–æ¨¡å‹ä¼šè¯
        self.model_session = None
        self._initialize_model_session()
        
        # é˜Ÿåˆ—ç³»ç»Ÿ
        self.inference_queue = queue.Queue(maxsize=10)
        self.postprocess_queue = queue.Queue(maxsize=10)  # ğŸ”§ æ·»åŠ åå¤„ç†é˜Ÿåˆ—
        self.output_queue = queue.Queue(maxsize=10)  # æœ€ç»ˆè¾“å‡ºé˜Ÿåˆ—
        
        # æ§åˆ¶å˜é‡
        self.running = False
        self.worker_threads = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'frames_received': 0,
            'frames_processed': 0,
            'inference_count': 0,
            'avg_inference_time': 0.0,
            'avg_postprocess_time': 0.0,
            'throughput_fps': 0.0,
            'frame_ordering_stats': {}
        }
        
        print(f"[SUCCESS] âœ… å¢å¼ºå¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_model_session(self):
        """åˆå§‹åŒ–æ¨¡å‹ä¼šè¯"""
        try:
            # é…ç½®ONNX Runtime
            providers = []
            if self.enable_gpu_inference:
                if 'CUDAExecutionProvider' in ort.get_available_providers():
                    providers.append('CUDAExecutionProvider')
                    print("[INFO] ğŸ® ä½¿ç”¨CUDA GPUæ¨ç†")
                elif 'DmlExecutionProvider' in ort.get_available_providers():
                    providers.append('DmlExecutionProvider')
                    print("[INFO] ğŸ® ä½¿ç”¨DirectML GPUæ¨ç†")
            
            providers.append('CPUExecutionProvider')
            
            # åˆ›å»ºä¼šè¯é€‰é¡¹
            sess_options = ort.SessionOptions()
            sess_options.inter_op_num_threads = self.num_inference_threads
            sess_options.intra_op_num_threads = self.num_inference_threads
            sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # åˆ›å»ºæ¨ç†ä¼šè¯
            self.model_session = ort.InferenceSession(
                self.model_path,
                sess_options=sess_options,
                providers=providers
            )
            
            print(f"[SUCCESS] âœ… æ¨¡å‹ä¼šè¯åˆå§‹åŒ–å®Œæˆ")
            print(f"   â€¢ æä¾›è€…: {self.model_session.get_providers()}")
            
        except Exception as e:
            print(f"[ERROR] âŒ æ¨¡å‹ä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def start(self):
        """å¯åŠ¨å¤„ç†ç³»ç»Ÿ"""
        if self.running:
            return
        
        self.running = True
        
        # å¯åŠ¨å·¥ä½œçº¿ç¨‹
        threads = [
            ('BatchCollector', self._batch_collector_worker),
            ('InferenceWorker', self._inference_worker),
            ('PostprocessWorker', self._postprocess_worker),
            ('FrameCleanup', self._frame_cleanup_worker),
            ('StatsWorker', self._stats_worker)
        ]
        
        for name, target in threads:
            thread = threading.Thread(target=target, daemon=True, name=name)
            thread.start()
            self.worker_threads.append(thread)
        
        print("[INFO] âœ… å¢å¼ºå¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿå·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢å¤„ç†ç³»ç»Ÿ"""
        self.running = False
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        for thread in self.worker_threads:
            thread.join(timeout=1.0)
        
        self.worker_threads.clear()
        print("[INFO] ğŸ›‘ å¢å¼ºå¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿå·²åœæ­¢")
    
    def process_frame_async(self, frame: np.ndarray, metadata: Dict = None) -> bool:
        """
        å¼‚æ­¥å¤„ç†å¸§ï¼ˆé›†æˆå¸§æ—¶é—´é¡ºåºç®¡ç†ï¼‰
        
        Args:
            frame: è¾“å…¥å¸§
            metadata: å…ƒæ•°æ®
            
        Returns:
            æ˜¯å¦æˆåŠŸæäº¤å¤„ç†
        """
        try:
            frame_data = {
                'frame': frame,
                'metadata': metadata or {},
                'timestamp': time.time(),
                'original_frame_id': self.stats['frames_received']
            }
            
            # ä½¿ç”¨å¸§æ—¶é—´é¡ºåºç®¡ç†å™¨
            success = self.frame_manager.add_frame(frame_data)
            if success:
                self.stats['frames_received'] += 1
            
            return success
            
        except Exception as e:
            print(f"[ERROR] å¸§æäº¤å¤±è´¥: {e}")
            return False
    
    def get_result(self, timeout: float = 0.001) -> Optional[Dict]:
        """è·å–å¤„ç†ç»“æœ"""
        try:
            return self.output_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def _batch_collector_worker(self):
        """æ‰¹å¤„ç†æ”¶é›†å·¥ä½œçº¿ç¨‹ï¼ˆä½¿ç”¨å¸§æ—¶é—´é¡ºåºç®¡ç†ï¼‰"""
        while self.running:
            try:
                # ä»å¸§ç®¡ç†å™¨è·å–æœ€æ–°å¸§
                batch_frames_data = self.frame_manager.get_batch_frames(self.batch_size)
                
                if not batch_frames_data:
                    time.sleep(0.001)
                    continue
                
                # å‡†å¤‡æ‰¹å¤„ç†æ•°æ®
                batch_frames = []
                batch_metadata = []
                
                for frame_data in batch_frames_data:
                    batch_frames.append(frame_data['frame'])
                    batch_metadata.append(frame_data)
                
                # å‡†å¤‡æ‰¹å¤„ç†è¾“å…¥
                if len(batch_frames) == 1:
                    batch_input = np.expand_dims(batch_frames[0], 0)
                else:
                    batch_input = np.stack(batch_frames, axis=0)
                
                # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ (NCHW)
                batch_input = self._prepare_model_input(batch_input)
                
                # æäº¤åˆ°æ¨ç†é˜Ÿåˆ—
                inference_data = {
                    'input': batch_input,
                    'metadata': batch_metadata,
                    'batch_size': len(batch_frames),
                    'timestamp': time.time()
                }
                
                try:
                    self.inference_queue.put_nowait(inference_data)
                except queue.Full:
                    # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„æ¨ç†ä»»åŠ¡
                    try:
                        self.inference_queue.get_nowait()
                        self.inference_queue.put_nowait(inference_data)
                    except queue.Empty:
                        pass
                
            except Exception as e:
                print(f"[ERROR] æ‰¹å¤„ç†æ”¶é›†çº¿ç¨‹é”™è¯¯: {e}")
                time.sleep(0.001)
    
    def _prepare_model_input(self, batch_input: np.ndarray) -> np.ndarray:
        """å‡†å¤‡æ¨¡å‹è¾“å…¥æ ¼å¼"""
        try:
            if batch_input.ndim == 4:
                # æ‰¹å¤„ç†æƒ…å†µ
                if batch_input.shape[-1] == 4:
                    # ç§»é™¤alphaé€šé“ (RGBA -> RGB)
                    batch_input = batch_input[:, :, :, :3]
                if batch_input.shape[-1] == 3:
                    # ä» (batch, H, W, C) è½¬æ¢ä¸º (batch, C, H, W)
                    batch_input = np.transpose(batch_input, (0, 3, 1, 2))
            elif batch_input.ndim == 3:
                # å•å¸§æƒ…å†µ
                if batch_input.shape[-1] == 4:
                    batch_input = batch_input[:, :, :3]
                if batch_input.shape[-1] == 3:
                    batch_input = np.transpose(batch_input, (2, 0, 1))
                batch_input = np.expand_dims(batch_input, 0)
            
            # å½’ä¸€åŒ–åˆ° [0, 1]
            if batch_input.dtype == np.uint8:
                batch_input = batch_input.astype(np.float32) / 255.0
            elif batch_input.dtype != np.float32:
                batch_input = batch_input.astype(np.float32)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šè½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›çš„float16ç±»å‹
            batch_input = batch_input.astype(np.float16)
            
            print(f"[DEBUG] æ¨¡å‹è¾“å…¥å‡†å¤‡å®Œæˆ - å½¢çŠ¶: {batch_input.shape}, æ•°æ®ç±»å‹: {batch_input.dtype}")
            
            return batch_input
            
        except Exception as e:
            print(f"[ERROR] æ¨¡å‹è¾“å…¥å‡†å¤‡å¤±è´¥: {e}")
            print(f"[ERROR] è¾“å…¥å½¢çŠ¶: {batch_input.shape if batch_input is not None else 'None'}")
            print(f"[ERROR] è¾“å…¥æ•°æ®ç±»å‹: {batch_input.dtype if batch_input is not None else 'None'}")
            return batch_input
    
    def _inference_worker(self):
        """æ¨ç†å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                # è·å–æ¨ç†ä»»åŠ¡
                inference_data = self.inference_queue.get(timeout=0.1)
                
                start_time = time.time()
                
                # æ‰§è¡Œæ¨ç†
                input_name = self.model_session.get_inputs()[0].name
                outputs = self.model_session.run(None, {input_name: inference_data['input']})
                
                inference_time = time.time() - start_time
                
                # æ›´æ–°ç»Ÿè®¡
                self.stats['inference_count'] += 1
                self.stats['avg_inference_time'] = (
                    self.stats['avg_inference_time'] * (self.stats['inference_count'] - 1) + inference_time
                ) / self.stats['inference_count']
                
                # æäº¤åˆ°åå¤„ç†
                postprocess_data = {
                    'outputs': outputs,
                    'metadata': inference_data['metadata'],
                    'batch_size': inference_data['batch_size'],
                    'inference_time': inference_time,
                    'timestamp': time.time()
                }
                
                try:
                    self.postprocess_queue.put_nowait(postprocess_data)  # ğŸ”§ ä¿®å¤ï¼šæ”¾å…¥åå¤„ç†é˜Ÿåˆ—
                except queue.Full:
                    # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„ç»“æœ
                    try:
                        self.postprocess_queue.get_nowait()
                        self.postprocess_queue.put_nowait(postprocess_data)
                    except queue.Empty:
                        pass
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"[ERROR] æ¨ç†çº¿ç¨‹é”™è¯¯: {e}")
                time.sleep(0.001)
    
    def _postprocess_worker(self):
        """åå¤„ç†å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                # è·å–æ¨ç†ç»“æœè¿›è¡Œåå¤„ç†
                postprocess_data = self.postprocess_queue.get(timeout=0.1)  # ğŸ”§ ä¿®å¤ï¼šä»åå¤„ç†é˜Ÿåˆ—è·å–
                
                # æ‰§è¡Œåå¤„ç†
                processed_results = self._postprocess_batch(
                    postprocess_data['outputs'],
                    postprocess_data['metadata'],
                    postprocess_data['batch_size']
                )
                
                # ä¸ºæ¯ä¸ªç»“æœåˆ›å»ºæœ€ç»ˆè¾“å‡º
                for i, result in enumerate(processed_results):
                    # è·å–å¯¹åº”çš„å…ƒæ•°æ®
                    frame_metadata = postprocess_data['metadata'][i] if i < len(postprocess_data['metadata']) else {}
                    
                    # è®¡ç®—å¸§å¹´é¾„ï¼ˆå¦‚æœæœ‰æ—¶é—´æˆ³ä¿¡æ¯ï¼‰
                    frame_age = 0
                    if 'timestamp' in frame_metadata:
                        frame_age = time.time() - frame_metadata['timestamp']
                    
                    final_output = {
                        'detections': result,  # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„é”®å
                        'metadata': frame_metadata,
                        'inference_time': postprocess_data['inference_time'],
                        'frame_age': frame_age,
                        'frame_id': frame_metadata.get('frame_id', 'unknown'),
                        'timestamp': postprocess_data['timestamp']
                    }
                    
                    # æ”¾å…¥æœ€ç»ˆè¾“å‡ºé˜Ÿåˆ—
                    try:
                        self.output_queue.put_nowait(final_output)  # ğŸ”§ ä¿®å¤ï¼šæ”¾å…¥è¾“å‡ºé˜Ÿåˆ—
                        self.stats['frames_processed'] += 1
                        
                    except queue.Full:
                        # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„ç»“æœ
                        try:
                            self.output_queue.get_nowait()
                            self.output_queue.put_nowait(final_output)
                        except queue.Empty:
                            pass
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"[ERROR] åå¤„ç†çº¿ç¨‹é”™è¯¯: {e}")
                time.sleep(0.001)
    
    def _frame_cleanup_worker(self):
        """å¸§æ¸…ç†å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                self.frame_manager.clear_old_frames()
                time.sleep(0.01)  # æ¯10msæ¸…ç†ä¸€æ¬¡
            except Exception as e:
                print(f"[ERROR] å¸§æ¸…ç†çº¿ç¨‹é”™è¯¯: {e}")
                time.sleep(0.1)
    
    def _stats_worker(self):
        """ç»Ÿè®¡å·¥ä½œçº¿ç¨‹"""
        last_stats_time = time.time()
        last_processed = 0
        
        while self.running:
            try:
                time.sleep(1.0)  # æ¯ç§’æ›´æ–°ç»Ÿè®¡
                
                current_time = time.time()
                time_diff = current_time - last_stats_time
                
                # è®¡ç®—ååé‡
                processed_diff = self.stats['frames_processed'] - last_processed
                self.stats['throughput_fps'] = processed_diff / time_diff
                
                # æ›´æ–°å¸§ç®¡ç†å™¨ç»Ÿè®¡
                self.stats['frame_ordering_stats'] = self.frame_manager.stats.copy()
                
                # æ›´æ–°è®°å½•
                last_stats_time = current_time
                last_processed = self.stats['frames_processed']
                
            except Exception as e:
                print(f"[ERROR] ç»Ÿè®¡çº¿ç¨‹é”™è¯¯: {e}")
    
    def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = self.stats.copy()
        
        # æ·»åŠ é˜Ÿåˆ—çŠ¶æ€
        stats['queue_status'] = {
            'inference_queue_size': self.inference_queue.qsize(),
            'postprocess_queue_size': self.postprocess_queue.qsize(),  # ğŸ”§ æ·»åŠ åå¤„ç†é˜Ÿåˆ—çŠ¶æ€
            'output_queue_size': self.output_queue.qsize(),
            'frame_buffer_size': self.frame_manager.get_buffer_size()
        }
        
        return stats
    
    def _postprocess_batch(self, outputs: List[np.ndarray], metadata: List[Dict], batch_size: int) -> List[pd.DataFrame]:
        """æ‰¹å¤„ç†åå¤„ç†"""
        results = []
        
        try:
            # æ£€æŸ¥outputsæ˜¯å¦æœ‰æ•ˆ
            if not outputs or len(outputs) == 0:
                print("[WARNING] åå¤„ç†æ”¶åˆ°ç©ºçš„outputs")
                for _ in range(batch_size):
                    results.append(pd.DataFrame(columns=['x1', 'y1', 'x2', 'y2', 'confidence', 'class', 'center_x', 'center_y', 'width', 'height']))
                return results
            
            # å‡è®¾è¾“å‡ºæ˜¯ [batch_size, num_detections, 6] æ ¼å¼
            predictions = outputs[0]  # ä¸»è¦è¾“å‡º
            
            for i in range(batch_size):
                # æå–å•ä¸ªæ ·æœ¬çš„é¢„æµ‹
                pred = predictions[i:i+1]  # ä¿æŒæ‰¹æ¬¡ç»´åº¦
                
                # åº”ç”¨NMS
                pred_nms = non_max_suppression(
                    torch.from_numpy(pred),
                    conf_thres=0.45,
                    iou_thres=0.45,
                    classes=None,
                    agnostic=False,
                    max_det=10
                )
                
                # è½¬æ¢ä¸ºDataFrame
                if pred_nms[0] is not None and len(pred_nms[0]) > 0:
                    detections = pred_nms[0].cpu().numpy()
                    
                    df = pd.DataFrame(detections, columns=['x1', 'y1', 'x2', 'y2', 'confidence', 'class'])
                    
                    # è®¡ç®—ä¸­å¿ƒç‚¹å’Œå°ºå¯¸
                    df['center_x'] = (df['x1'] + df['x2']) / 2
                    df['center_y'] = (df['y1'] + df['y2']) / 2
                    df['width'] = df['x2'] - df['x1']
                    df['height'] = df['y2'] - df['y1']
                    
                    results.append(df)
                else:
                    # ç©ºæ£€æµ‹ç»“æœ
                    results.append(pd.DataFrame(columns=['x1', 'y1', 'x2', 'y2', 'confidence', 'class', 'center_x', 'center_y', 'width', 'height']))
            
        except Exception as e:
            print(f"[ERROR] æ‰¹å¤„ç†åå¤„ç†å¤±è´¥: {e}")
            # è¿”å›ç©ºç»“æœ
            for _ in range(batch_size):
                results.append(pd.DataFrame(columns=['x1', 'y1', 'x2', 'y2', 'confidence', 'class', 'center_x', 'center_y', 'width', 'height']))
        
        return results
    
    def print_performance_stats(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡"""
        stats = self.get_performance_stats()
        frame_stats = stats['frame_ordering_stats']
        queue_status = stats['queue_status']
        
        print(f"\nğŸš€ å¢å¼ºå¤šçº¿ç¨‹AIå¤„ç†å™¨ç»Ÿè®¡:")
        print(f"   â€¢ æ¥æ”¶å¸§æ•°: {stats['frames_received']}")
        print(f"   â€¢ å¤„ç†å¸§æ•°: {stats['frames_processed']}")
        print(f"   â€¢ æ¨ç†æ¬¡æ•°: {stats['inference_count']}")
        print(f"   â€¢ å¹³å‡æ¨ç†æ—¶é—´: {stats['avg_inference_time']*1000:.1f}ms")
        print(f"   â€¢ ååé‡: {stats['throughput_fps']:.1f} FPS")
        
        print(f"\nğŸ•’ å¸§æ—¶é—´é¡ºåºç®¡ç†ç»Ÿè®¡:")
        print(f"   â€¢ å¸§ç®¡ç†å™¨æ¥æ”¶: {frame_stats.get('frames_received', 0)}")
        print(f"   â€¢ å¸§ç®¡ç†å™¨å¤„ç†: {frame_stats.get('frames_processed', 0)}")
        print(f"   â€¢ ä¸¢å¼ƒè¿‡æ—¶å¸§: {frame_stats.get('frames_discarded_old', 0)}")
        print(f"   â€¢ ä¸¢å¼ƒæº¢å‡ºå¸§: {frame_stats.get('frames_discarded_overflow', 0)}")
        print(f"   â€¢ å¹³å‡å¸§å¹´é¾„: {frame_stats.get('avg_frame_age', 0)*1000:.1f}ms")
        
        print(f"\nğŸ“Š é˜Ÿåˆ—çŠ¶æ€:")
        print(f"   â€¢ æ¨ç†é˜Ÿåˆ—: {queue_status['inference_queue_size']}")
        print(f"   â€¢ åå¤„ç†é˜Ÿåˆ—: {queue_status['postprocess_queue_size']}")  # ğŸ”§ æ·»åŠ åå¤„ç†é˜Ÿåˆ—æ˜¾ç¤º
        print(f"   â€¢ è¾“å‡ºé˜Ÿåˆ—: {queue_status['output_queue_size']}")
        print(f"   â€¢ å¸§ç¼“å†²åŒº: {queue_status['frame_buffer_size']}")


def test_enhanced_processor():
    """æµ‹è¯•å¢å¼ºå¤„ç†å™¨"""
    print("ğŸ§ª æµ‹è¯•å¢å¼ºå¤šçº¿ç¨‹AIå¤„ç†å™¨")
    print("=" * 60)
    
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å®é™…çš„æ¨¡å‹è·¯å¾„
    # processor = EnhancedMultiThreadedAIProcessor(
    #     model_path="path/to/your/model.onnx",
    #     batch_size=2,
    #     max_frame_age=0.1
    # )
    
    print("âœ… å¢å¼ºå¤„ç†å™¨æµ‹è¯•æ¡†æ¶å‡†å¤‡å®Œæˆ")
    print("ğŸ’¡ éœ€è¦å®é™…æ¨¡å‹è·¯å¾„æ‰èƒ½å®Œæ•´æµ‹è¯•")


if __name__ == "__main__":
    test_enhanced_processor()