#!/usr/bin/env python3
"""
YOLOv8 vs ONNX æ¨¡å‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•
å¯¹æ¯”YOLOv8 PTæ¨¡å‹ä¸å½“å‰ONNXæ¨¡å‹çš„æ€§èƒ½å·®å¼‚
"""

import time
import numpy as np
import cv2
import torch
import onnxruntime as ort
import psutil
import gc
import os
from pathlib import Path
import json
from datetime import datetime

# å°è¯•å¯¼å…¥YOLOv8
try:
    from ultralytics import YOLO
    YOLOV8_AVAILABLE = True
except ImportError:
    YOLOV8_AVAILABLE = False
    print("âš ï¸ YOLOv8 (ultralytics) æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install ultralytics")

class ModelBenchmark:
    """æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.results = {
            'onnx': {},
            'yolov8': {},
            'comparison': {}
        }
        
        # æµ‹è¯•é…ç½®
        self.test_config = {
            'num_warmup': 10,      # é¢„çƒ­æ¬¡æ•°
            'num_iterations': 100,  # æµ‹è¯•è¿­ä»£æ¬¡æ•°
            'input_size': (320, 320),  # è¾“å…¥å°ºå¯¸
            'batch_size': 1,
            'confidence': 0.3,
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }
        
        print(f"ğŸ”§ æµ‹è¯•é…ç½®: {self.test_config}")
    
    def create_test_input(self, size=(320, 320), use_fp16=False):
        """åˆ›å»ºæµ‹è¯•è¾“å…¥æ•°æ®"""
        # åˆ›å»ºéšæœºå›¾åƒæ•°æ®
        img = np.random.randint(0, 255, (size[1], size[0], 3), dtype=np.uint8)
        
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
        img_tensor = (torch.from_numpy(img).float().to('cuda') / 255.0).cpu().numpy()
        img_tensor = np.transpose(img_tensor, (2, 0, 1))  # HWC -> CHW
        img_tensor = np.expand_dims(img_tensor, axis=0)   # æ·»åŠ batchç»´åº¦
        
        if use_fp16:
            img_tensor = img_tensor.astype(np.float16)
        
        return img, img_tensor
    
    def get_system_info(self):
        """è·å–ç³»ç»Ÿä¿¡æ¯"""
        info = {
            'cpu': psutil.cpu_count(),
            'memory_gb': psutil.virtual_memory().total / (1024**3),
            'gpu_available': torch.cuda.is_available(),
        }
        
        if torch.cuda.is_available():
            info['gpu_name'] = torch.cuda.get_device_name(0)
            info['gpu_memory_gb'] = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        
        return info
    
    def benchmark_onnx_model(self, model_path):
        """æµ‹è¯•ONNXæ¨¡å‹æ€§èƒ½"""
        print(f"\nğŸ”„ æµ‹è¯•ONNXæ¨¡å‹: {model_path}")
        
        if not os.path.exists(model_path):
            print(f"âŒ ONNXæ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
        
        try:
            # åˆ›å»ºONNXä¼šè¯
            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']
            session = ort.InferenceSession(model_path, providers=providers)
            
            # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
            input_name = session.get_inputs()[0].name
            output_names = [output.name for output in session.get_outputs()]
            
            print(f"âœ… ONNXä¼šè¯åˆ›å»ºæˆåŠŸ")
            print(f"ğŸ“Š è¾“å…¥: {input_name}, è¾“å‡º: {output_names}")
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_img, test_input = self.create_test_input(
                self.test_config['input_size'], 
                use_fp16=True  # ONNXæ¨¡å‹ä½¿ç”¨åŠç²¾åº¦
            )
            
            # é¢„çƒ­
            print("ğŸ”¥ é¢„çƒ­ONNXæ¨¡å‹...")
            for _ in range(self.test_config['num_warmup']):
                _ = session.run(output_names, {input_name: test_input})
            
            # æ€§èƒ½æµ‹è¯•
            print("ğŸ“Š å¼€å§‹ONNXæ€§èƒ½æµ‹è¯•...")
            start_memory = psutil.virtual_memory().used / (1024**2)
            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats()
                start_gpu_memory = torch.cuda.memory_allocated() / (1024**2)
            
            inference_times = []
            start_time = time.time()
            
            for i in range(self.test_config['num_iterations']):
                iter_start = time.time()
                outputs = session.run(output_names, {input_name: test_input})
                iter_end = time.time()
                inference_times.append((iter_end - iter_start) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                
                if (i + 1) % 20 == 0:
                    print(f"  è¿›åº¦: {i+1}/{self.test_config['num_iterations']}")
            
            total_time = time.time() - start_time
            
            # è®¡ç®—å†…å­˜ä½¿ç”¨
            end_memory = psutil.virtual_memory().used / (1024**2)
            memory_increase = end_memory - start_memory
            
            gpu_memory_increase = 0
            if torch.cuda.is_available():
                end_gpu_memory = torch.cuda.memory_allocated() / (1024**2)
                gpu_memory_increase = end_gpu_memory - start_gpu_memory
            
            # ç»Ÿè®¡ç»“æœ
            results = {
                'model_path': model_path,
                'avg_inference_time_ms': np.mean(inference_times),
                'min_inference_time_ms': np.min(inference_times),
                'max_inference_time_ms': np.max(inference_times),
                'std_inference_time_ms': np.std(inference_times),
                'fps': 1000 / np.mean(inference_times),
                'total_time_s': total_time,
                'memory_increase_mb': memory_increase,
                'gpu_memory_increase_mb': gpu_memory_increase,
                'iterations': self.test_config['num_iterations']
            }
            
            print(f"âœ… ONNXæµ‹è¯•å®Œæˆ")
            print(f"ğŸ“Š å¹³å‡æ¨ç†æ—¶é—´: {results['avg_inference_time_ms']:.2f}ms")
            print(f"ğŸ“Š FPS: {results['fps']:.1f}")
            
            return results
            
        except Exception as e:
            print(f"âŒ ONNXæµ‹è¯•å¤±è´¥: {e}")
            return None
    
    def benchmark_yolov8_model(self, model_path):
        """æµ‹è¯•YOLOv8æ¨¡å‹æ€§èƒ½"""
        print(f"\nğŸ”„ æµ‹è¯•YOLOv8æ¨¡å‹: {model_path}")
        
        if not YOLOV8_AVAILABLE:
            print("âŒ YOLOv8ä¸å¯ç”¨")
            return None
        
        if not os.path.exists(model_path):
            print(f"âŒ YOLOv8æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
            return None
        
        try:
            # åŠ è½½YOLOv8æ¨¡å‹
            model = YOLO(model_path)
            
            # ç§»åŠ¨åˆ°GPU
            if self.test_config['device'] == 'cuda' and torch.cuda.is_available():
                model = model.cuda()
                if hasattr(model.model, 'half'):
                    model = model.half()
            
            print(f"âœ… YOLOv8æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            test_img, _ = self.create_test_input(self.test_config['input_size'])
            
            # é¢„çƒ­
            print("ğŸ”¥ é¢„çƒ­YOLOv8æ¨¡å‹...")
            for _ in range(self.test_config['num_warmup']):
                _ = model.predict(
                    test_img,
                    device=self.test_config['device'],
                    verbose=False,
                    conf=self.test_config['confidence']
                )
            
            # æ€§èƒ½æµ‹è¯•
            print("ğŸ“Š å¼€å§‹YOLOv8æ€§èƒ½æµ‹è¯•...")
            start_memory = psutil.virtual_memory().used / (1024**2)
            if torch.cuda.is_available():
                torch.cuda.reset_peak_memory_stats()
                start_gpu_memory = torch.cuda.memory_allocated() / (1024**2)
            
            inference_times = []
            start_time = time.time()
            
            for i in range(self.test_config['num_iterations']):
                iter_start = time.time()
                results = model.predict(
                    test_img,
                    device=self.test_config['device'],
                    verbose=False,
                    conf=self.test_config['confidence'],
                    classes=[0]  # åªæ£€æµ‹äººç‰©
                )
                iter_end = time.time()
                inference_times.append((iter_end - iter_start) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
                
                if (i + 1) % 20 == 0:
                    print(f"  è¿›åº¦: {i+1}/{self.test_config['num_iterations']}")
            
            total_time = time.time() - start_time
            
            # è®¡ç®—å†…å­˜ä½¿ç”¨
            end_memory = psutil.virtual_memory().used / (1024**2)
            memory_increase = end_memory - start_memory
            
            gpu_memory_increase = 0
            if torch.cuda.is_available():
                end_gpu_memory = torch.cuda.memory_allocated() / (1024**2)
                gpu_memory_increase = end_gpu_memory - start_gpu_memory
            
            # ç»Ÿè®¡ç»“æœ
            results = {
                'model_path': model_path,
                'avg_inference_time_ms': np.mean(inference_times),
                'min_inference_time_ms': np.min(inference_times),
                'max_inference_time_ms': np.max(inference_times),
                'std_inference_time_ms': np.std(inference_times),
                'fps': 1000 / np.mean(inference_times),
                'total_time_s': total_time,
                'memory_increase_mb': memory_increase,
                'gpu_memory_increase_mb': gpu_memory_increase,
                'iterations': self.test_config['num_iterations']
            }
            
            print(f"âœ… YOLOv8æµ‹è¯•å®Œæˆ")
            print(f"ğŸ“Š å¹³å‡æ¨ç†æ—¶é—´: {results['avg_inference_time_ms']:.2f}ms")
            print(f"ğŸ“Š FPS: {results['fps']:.1f}")
            
            return results
            
        except Exception as e:
            print(f"âŒ YOLOv8æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    def compare_results(self, onnx_results, yolov8_results):
        """å¯¹æ¯”æµ‹è¯•ç»“æœ"""
        if not onnx_results or not yolov8_results:
            print("âŒ æ— æ³•è¿›è¡Œå¯¹æ¯”ï¼Œç¼ºå°‘æµ‹è¯•ç»“æœ")
            return None
        
        print("\n" + "="*60)
        print("ğŸ“Š YOLOv8 vs ONNX æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        print("="*60)
        
        # é€Ÿåº¦å¯¹æ¯”
        onnx_fps = onnx_results['fps']
        yolov8_fps = yolov8_results['fps']
        speed_diff = ((yolov8_fps - onnx_fps) / onnx_fps) * 100
        
        print(f"\nğŸš€ æ¨ç†é€Ÿåº¦å¯¹æ¯”:")
        print(f"  ONNXæ¨¡å‹:    {onnx_fps:.1f} FPS ({onnx_results['avg_inference_time_ms']:.2f}ms)")
        print(f"  YOLOv8æ¨¡å‹:  {yolov8_fps:.1f} FPS ({yolov8_results['avg_inference_time_ms']:.2f}ms)")
        print(f"  é€Ÿåº¦å·®å¼‚:    {speed_diff:+.1f}% ({'YOLOv8æ›´å¿«' if speed_diff > 0 else 'ONNXæ›´å¿«'})")
        
        # å†…å­˜å¯¹æ¯”
        onnx_mem = onnx_results['memory_increase_mb']
        yolov8_mem = yolov8_results['memory_increase_mb']
        mem_diff = yolov8_mem - onnx_mem
        
        print(f"\nğŸ’¾ å†…å­˜ä½¿ç”¨å¯¹æ¯”:")
        print(f"  ONNXæ¨¡å‹:    {onnx_mem:+.1f} MB")
        print(f"  YOLOv8æ¨¡å‹:  {yolov8_mem:+.1f} MB")
        print(f"  å†…å­˜å·®å¼‚:    {mem_diff:+.1f} MB")
        
        # GPUå†…å­˜å¯¹æ¯”
        if torch.cuda.is_available():
            onnx_gpu_mem = onnx_results['gpu_memory_increase_mb']
            yolov8_gpu_mem = yolov8_results['gpu_memory_increase_mb']
            gpu_mem_diff = yolov8_gpu_mem - onnx_gpu_mem
            
            print(f"\nğŸ® GPUå†…å­˜ä½¿ç”¨å¯¹æ¯”:")
            print(f"  ONNXæ¨¡å‹:    {onnx_gpu_mem:+.1f} MB")
            print(f"  YOLOv8æ¨¡å‹:  {yolov8_gpu_mem:+.1f} MB")
            print(f"  GPUå†…å­˜å·®å¼‚: {gpu_mem_diff:+.1f} MB")
        
        # ç¨³å®šæ€§å¯¹æ¯”
        onnx_std = onnx_results['std_inference_time_ms']
        yolov8_std = yolov8_results['std_inference_time_ms']
        
        print(f"\nğŸ“ˆ æ¨ç†ç¨³å®šæ€§å¯¹æ¯”:")
        print(f"  ONNXæ¨¡å‹:    æ ‡å‡†å·® {onnx_std:.2f}ms")
        print(f"  YOLOv8æ¨¡å‹:  æ ‡å‡†å·® {yolov8_std:.2f}ms")
        print(f"  ç¨³å®šæ€§:      {'YOLOv8æ›´ç¨³å®š' if yolov8_std < onnx_std else 'ONNXæ›´ç¨³å®š'}")
        
        # å»ºè®®
        print(f"\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        if speed_diff > 10:
            print("  âœ… æ¨èä½¿ç”¨YOLOv8æ¨¡å‹ - é€Ÿåº¦ä¼˜åŠ¿æ˜æ˜¾")
        elif speed_diff < -10:
            print("  âœ… æ¨èä½¿ç”¨ONNXæ¨¡å‹ - é€Ÿåº¦ä¼˜åŠ¿æ˜æ˜¾")
        else:
            print("  âš–ï¸ ä¸¤ä¸ªæ¨¡å‹æ€§èƒ½ç›¸è¿‘ï¼Œå¯æ ¹æ®å…¶ä»–å› ç´ é€‰æ‹©")
        
        if abs(mem_diff) > 100:
            print(f"  âš ï¸ æ³¨æ„å†…å­˜å·®å¼‚è¾ƒå¤§: {abs(mem_diff):.1f}MB")
        
        # ä¿å­˜å¯¹æ¯”ç»“æœ
        comparison = {
            'timestamp': datetime.now().isoformat(),
            'onnx_results': onnx_results,
            'yolov8_results': yolov8_results,
            'speed_difference_percent': speed_diff,
            'memory_difference_mb': mem_diff,
            'recommendation': 'yolov8' if speed_diff > 5 else 'onnx' if speed_diff < -5 else 'similar'
        }
        
        return comparison
    
    def save_results(self, results, filename='benchmark_results.json'):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")
    
    def run_full_benchmark(self):
        """è¿è¡Œå®Œæ•´çš„åŸºå‡†æµ‹è¯•"""
        print("ğŸ¯ YOLOv8 vs ONNX æ¨¡å‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("="*50)
        
        # ç³»ç»Ÿä¿¡æ¯
        system_info = self.get_system_info()
        print(f"\nğŸ’» ç³»ç»Ÿä¿¡æ¯:")
        for key, value in system_info.items():
            print(f"  {key}: {value}")
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # æµ‹è¯•ONNXæ¨¡å‹
        onnx_models = ['yolov5s320Half.onnx', 'yolov5m320Half.onnx']
        onnx_results = None
        
        for model_path in onnx_models:
            if os.path.exists(model_path):
                onnx_results = self.benchmark_onnx_model(model_path)
                break
        
        # æ¸…ç†å†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # æµ‹è¯•YOLOv8æ¨¡å‹
        yolov8_models = ['best.pt', 'models/valorant/best.pt', 'yolov8s.pt']
        yolov8_results = None
        
        for model_path in yolov8_models:
            if os.path.exists(model_path):
                yolov8_results = self.benchmark_yolov8_model(model_path)
                break
        
        # å¯¹æ¯”ç»“æœ
        comparison = self.compare_results(onnx_results, yolov8_results)
        
        # ä¿å­˜ç»“æœ
        all_results = {
            'system_info': system_info,
            'test_config': self.test_config,
            'onnx_results': onnx_results,
            'yolov8_results': yolov8_results,
            'comparison': comparison
        }
        
        self.save_results(all_results)
        
        print("\n" + "="*60)
        print("âœ… åŸºå‡†æµ‹è¯•å®Œæˆï¼")
        print("="*60)

def main():
    """ä¸»å‡½æ•°"""
    benchmark = ModelBenchmark()
    benchmark.run_full_benchmark()

if __name__ == "__main__":
    main()