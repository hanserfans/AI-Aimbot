#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUåˆ©ç”¨ç‡ä¼˜åŒ–å™¨
è§£å†³GPUä½¿ç”¨ç‡ä½ï¼ˆ21%ï¼‰å’Œå†…å­˜ä½¿ç”¨ç‡é«˜ï¼ˆ87.7%ï¼‰çš„é—®é¢˜
"""

import torch
import numpy as np
import cv2
import time
import json
import os
import gc
from typing import Tuple, Optional
import mss

class GPUUtilizationOptimizer:
    """GPUåˆ©ç”¨ç‡ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.stream = torch.cuda.Stream() if torch.cuda.is_available() else None
        
        # GPUå†…å­˜æ± 
        self.gpu_memory_pool = {}
        self.prealloc_tensors = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'gpu_preprocess_time': [],
            'gpu_inference_time': [],
            'gpu_postprocess_time': [],
            'total_gpu_time': [],
            'memory_usage': []
        }
        
        print(f"[INFO] ğŸš€ GPUåˆ©ç”¨ç‡ä¼˜åŒ–å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"[INFO] ğŸ“± ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        if torch.cuda.is_available():
            print(f"[INFO] ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")
            self.setup_gpu_memory_pool()
    
    def setup_gpu_memory_pool(self):
        """è®¾ç½®GPUå†…å­˜æ± ï¼Œé¢„åˆ†é…å¸¸ç”¨å°ºå¯¸çš„å¼ é‡"""
        try:
            # é¢„åˆ†é…å¸¸ç”¨å°ºå¯¸çš„å¼ é‡
            common_sizes = [
                (320, 320, 3),    # æ£€æµ‹å›¾åƒ
                (640, 640, 3),    # é«˜åˆ†è¾¨ç‡å›¾åƒ
                (1920, 1080, 3),  # å…¨å±æˆªå›¾
                (1, 3, 320, 320), # æ¨¡å‹è¾“å…¥
                (1, 3, 640, 640)  # é«˜åˆ†è¾¨ç‡æ¨¡å‹è¾“å…¥
            ]
            
            for size in common_sizes:
                key = f"tensor_{size}"
                if len(size) == 3:  # HWCæ ¼å¼
                    self.prealloc_tensors[key] = torch.zeros(size, dtype=torch.uint8, device=self.device)
                else:  # NCHWæ ¼å¼
                    self.prealloc_tensors[key] = torch.zeros(size, dtype=torch.float32, device=self.device)
            
            print(f"[INFO] ğŸŠ GPUå†…å­˜æ± è®¾ç½®å®Œæˆï¼Œé¢„åˆ†é… {len(common_sizes)} ä¸ªå¼ é‡")
            
        except Exception as e:
            print(f"[WARNING] GPUå†…å­˜æ± è®¾ç½®å¤±è´¥: {e}")
    
    def get_gpu_tensor(self, shape: Tuple, dtype=torch.uint8) -> torch.Tensor:
        """ä»å†…å­˜æ± è·å–GPUå¼ é‡"""
        key = f"tensor_{shape}"
        
        if key in self.prealloc_tensors:
            return self.prealloc_tensors[key]
        else:
            # åŠ¨æ€åˆ†é…
            return torch.zeros(shape, dtype=dtype, device=self.device)
    
    def gpu_screen_capture_optimized(self, region: Tuple[int, int, int, int]) -> Optional[torch.Tensor]:
        """GPUä¼˜åŒ–çš„å±å¹•æˆªå›¾"""
        try:
            start_time = time.time()
            
            # ä½¿ç”¨mssè¿›è¡Œå¿«é€Ÿæˆªå›¾
            with mss.mss() as sct:
                monitor = {
                    "top": region[1],
                    "left": region[0], 
                    "width": region[2],
                    "height": region[3]
                }
                screenshot = sct.grab(monitor)
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                img_np = torch.tensor(screenshot, device='cuda').cpu().numpy()[:, :, :3]  # ç§»é™¤alphaé€šé“
                
                # ç›´æ¥åœ¨GPUä¸Šåˆ›å»ºå¼ é‡
                img_tensor = torch.from_numpy(img_np).to(self.device, non_blocking=True)
                
                capture_time = time.time() - start_time
                print(f"[DEBUG] ğŸ“¸ GPUå±å¹•æˆªå›¾: {capture_time*1000:.2f}ms")
                
                return img_tensor
                
        except Exception as e:
            print(f"[ERROR] GPUå±å¹•æˆªå›¾å¤±è´¥: {e}")
            return None
    
    def gpu_image_preprocessing(self, img_tensor: torch.Tensor, target_size: Tuple[int, int] = (320, 320)) -> torch.Tensor:
        """GPUå›¾åƒé¢„å¤„ç†"""
        try:
            start_time = time.time()
            
            with torch.cuda.stream(self.stream) if self.stream else torch.no_grad():
                # ç¡®ä¿å¼ é‡åœ¨GPUä¸Š
                if img_tensor.device != self.device:
                    img_tensor = img_tensor.to(self.device, non_blocking=True)
                
                # è½¬æ¢æ•°æ®ç±»å‹
                img_float = img_tensor.float() / 255.0
                
                # è°ƒæ•´å°ºå¯¸ (ä½¿ç”¨åŒçº¿æ€§æ’å€¼)
                img_resized = torch.nn.functional.interpolate(
                    img_float.permute(2, 0, 1).unsqueeze(0),  # HWC -> NCHW
                    size=target_size,
                    mode='bilinear',
                    align_corners=False
                )
                
                # å½’ä¸€åŒ– (ImageNetæ ‡å‡†)
                mean = torch.tensor([0.485, 0.456, 0.406], device=self.device).view(1, 3, 1, 1)
                std = torch.tensor([0.229, 0.224, 0.225], device=self.device).view(1, 3, 1, 1)
                img_normalized = (img_resized - mean) / std
                
                preprocess_time = time.time() - start_time
                self.stats['gpu_preprocess_time'].append(preprocess_time * 1000)
                
                print(f"[DEBUG] ğŸ”„ GPUå›¾åƒé¢„å¤„ç†: {preprocess_time*1000:.2f}ms")
                
                return img_normalized
                
        except Exception as e:
            print(f"[ERROR] GPUå›¾åƒé¢„å¤„ç†å¤±è´¥: {e}")
            return None
    
    def gpu_postprocessing(self, model_output: torch.Tensor, conf_threshold: float = 0.4) -> torch.Tensor:
        """GPUåå¤„ç†"""
        try:
            start_time = time.time()
            
            with torch.cuda.stream(self.stream) if self.stream else torch.no_grad():
                # ç¡®ä¿è¾“å‡ºåœ¨GPUä¸Š
                if model_output.device != self.device:
                    model_output = model_output.to(self.device, non_blocking=True)
                
                # ç½®ä¿¡åº¦è¿‡æ»¤
                conf_mask = model_output[..., 4] > conf_threshold
                filtered_detections = model_output[conf_mask]
                
                if filtered_detections.numel() == 0:
                    return torch.empty((0, 6), device=self.device)
                
                # NMS (éæå¤§å€¼æŠ‘åˆ¶)
                boxes = filtered_detections[:, :4]
                scores = filtered_detections[:, 4]
                
                # ç®€åŒ–çš„NMSå®ç°
                keep_indices = torch.ops.torchvision.nms(boxes, scores, 0.45)
                final_detections = filtered_detections[keep_indices]
                
                postprocess_time = time.time() - start_time
                self.stats['gpu_postprocess_time'].append(postprocess_time * 1000)
                
                print(f"[DEBUG] ğŸ¯ GPUåå¤„ç†: {postprocess_time*1000:.2f}ms, æ£€æµ‹åˆ° {len(final_detections)} ä¸ªç›®æ ‡")
                
                return final_detections
                
        except Exception as e:
            print(f"[ERROR] GPUåå¤„ç†å¤±è´¥: {e}")
            return torch.empty((0, 6), device=self.device)
    
    def optimize_gpu_pipeline(self, region: Tuple[int, int, int, int], model_session) -> Optional[torch.Tensor]:
        """å®Œæ•´çš„GPUä¼˜åŒ–ç®¡é“"""
        try:
            pipeline_start = time.time()
            
            # 1. GPUå±å¹•æˆªå›¾
            img_tensor = self.gpu_screen_capture_optimized(region)
            if img_tensor is None:
                return None
            
            # 2. GPUå›¾åƒé¢„å¤„ç†
            preprocessed = self.gpu_image_preprocessing(img_tensor)
            if preprocessed is None:
                return None
            
            # 3. æ¨¡å‹æ¨ç† (åœ¨GPUä¸Š)
            inference_start = time.time()
            
            # è½¬æ¢ä¸ºnumpyè¿›è¡ŒONNXæ¨ç†
            input_np = preprocessed.cpu().numpy()
            outputs = model_session.run(None, {"images": input_np})
            
            # è½¬æ¢å›GPUå¼ é‡
            output_tensor = torch.from_numpy(outputs[0]).to(self.device)
            
            inference_time = time.time() - inference_start
            self.stats['gpu_inference_time'].append(inference_time * 1000)
            
            # 4. GPUåå¤„ç†
            detections = self.gpu_postprocessing(output_tensor)
            
            total_time = time.time() - pipeline_start
            self.stats['total_gpu_time'].append(total_time * 1000)
            
            # è®°å½•GPUå†…å­˜ä½¿ç”¨
            if torch.cuda.is_available():
                memory_used = torch.cuda.memory_allocated() / 1024**2  # MB
                self.stats['memory_usage'].append(memory_used)
            
            print(f"[INFO] âš¡ GPUç®¡é“æ€»æ—¶é—´: {total_time*1000:.2f}ms")
            
            return detections
            
        except Exception as e:
            print(f"[ERROR] GPUä¼˜åŒ–ç®¡é“å¤±è´¥: {e}")
            return None
    
    def get_performance_stats(self) -> dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        if not self.stats['total_gpu_time']:
            return {}
        
        def avg(lst):
            return sum(lst) / len(lst) if lst else 0
        
        return {
            'avg_preprocess_time_ms': avg(self.stats['gpu_preprocess_time']),
            'avg_inference_time_ms': avg(self.stats['gpu_inference_time']),
            'avg_postprocess_time_ms': avg(self.stats['gpu_postprocess_time']),
            'avg_total_time_ms': avg(self.stats['total_gpu_time']),
            'avg_memory_usage_mb': avg(self.stats['memory_usage']),
            'total_frames_processed': len(self.stats['total_gpu_time'])
        }
    
    def print_optimization_report(self):
        """æ‰“å°ä¼˜åŒ–æŠ¥å‘Š"""
        stats = self.get_performance_stats()
        
        if not stats:
            print("[INFO] ğŸ“Š æš‚æ— æ€§èƒ½æ•°æ®")
            return
        
        print("\n" + "="*50)
        print("ğŸš€ GPUåˆ©ç”¨ç‡ä¼˜åŒ–æŠ¥å‘Š")
        print("="*50)
        
        print(f"ğŸ“Š æ€§èƒ½ç»Ÿè®¡ (å¤„ç†äº† {stats['total_frames_processed']} å¸§):")
        print(f"  â€¢ å¹³å‡é¢„å¤„ç†æ—¶é—´: {stats['avg_preprocess_time_ms']:.2f}ms")
        print(f"  â€¢ å¹³å‡æ¨ç†æ—¶é—´: {stats['avg_inference_time_ms']:.2f}ms") 
        print(f"  â€¢ å¹³å‡åå¤„ç†æ—¶é—´: {stats['avg_postprocess_time_ms']:.2f}ms")
        print(f"  â€¢ å¹³å‡æ€»å¤„ç†æ—¶é—´: {stats['avg_total_time_ms']:.2f}ms")
        print(f"  â€¢ å¹³å‡GPUå†…å­˜ä½¿ç”¨: {stats['avg_memory_usage_mb']:.1f}MB")
        
        # è®¡ç®—ç†è®ºFPS
        if stats['avg_total_time_ms'] > 0:
            theoretical_fps = 1000 / stats['avg_total_time_ms']
            print(f"  â€¢ ç†è®ºæœ€å¤§FPS: {theoretical_fps:.1f}")
        
        # GPUåˆ©ç”¨ç‡ä¼°ç®—
        total_compute_time = (stats['avg_preprocess_time_ms'] + 
                            stats['avg_inference_time_ms'] + 
                            stats['avg_postprocess_time_ms'])
        
        if total_compute_time > 0:
            gpu_utilization = (total_compute_time / stats['avg_total_time_ms']) * 100
            print(f"  â€¢ ä¼°ç®—GPUåˆ©ç”¨ç‡: {gpu_utilization:.1f}%")
        
        print("\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        if stats['avg_total_time_ms'] > 50:
            print("  â€¢ è€ƒè™‘é™ä½å›¾åƒåˆ†è¾¨ç‡ä»¥æé«˜å¤„ç†é€Ÿåº¦")
        if stats['avg_memory_usage_mb'] < 1000:
            print("  â€¢ GPUå†…å­˜ä½¿ç”¨è¾ƒä½ï¼Œå¯ä»¥å¢åŠ æ‰¹å¤„ç†å¤§å°")
        if stats['avg_preprocess_time_ms'] > stats['avg_inference_time_ms']:
            print("  â€¢ é¢„å¤„ç†æ—¶é—´è¾ƒé•¿ï¼Œè€ƒè™‘è¿›ä¸€æ­¥ä¼˜åŒ–å›¾åƒå¤„ç†")
        
        print("="*50)
    
    def cleanup(self):
        """æ¸…ç†GPUèµ„æº"""
        try:
            # æ¸…ç†é¢„åˆ†é…çš„å¼ é‡
            for tensor in self.prealloc_tensors.values():
                del tensor
            self.prealloc_tensors.clear()
            
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
            
            gc.collect()
            print("[INFO] ğŸ§¹ GPUèµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"[WARNING] GPUèµ„æºæ¸…ç†å¤±è´¥: {e}")

def test_gpu_optimization():
    """æµ‹è¯•GPUä¼˜åŒ–æ•ˆæœ"""
    print("ğŸ§ª GPUåˆ©ç”¨ç‡ä¼˜åŒ–æµ‹è¯•")
    print("="*40)
    
    optimizer = GPUUtilizationOptimizer()
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUä¼˜åŒ–æµ‹è¯•")
        return
    
    try:
        # æ¨¡æ‹Ÿæ£€æµ‹åŒºåŸŸ (å±å¹•ä¸­å¿ƒ320x320)
        screen_width = 1920
        screen_height = 1080
        region_size = 320
        
        region = (
            (screen_width - region_size) // 2,
            (screen_height - region_size) // 2,
            region_size,
            region_size
        )
        
        print(f"ğŸ“ æµ‹è¯•åŒºåŸŸ: {region}")
        
        # è¿è¡Œæµ‹è¯•
        test_frames = 10
        print(f"ğŸ”„ å¤„ç† {test_frames} å¸§è¿›è¡Œæµ‹è¯•...")
        
        for i in range(test_frames):
            print(f"å¤„ç†ç¬¬ {i+1}/{test_frames} å¸§...")
            
            # æ¨¡æ‹ŸGPUä¼˜åŒ–ç®¡é“ (ä¸åŒ…å«å®é™…æ¨¡å‹æ¨ç†)
            img_tensor = optimizer.gpu_screen_capture_optimized(region)
            if img_tensor is not None:
                preprocessed = optimizer.gpu_image_preprocessing(img_tensor)
                if preprocessed is not None:
                    # æ¨¡æ‹Ÿåå¤„ç†
                    fake_output = torch.randn(1, 25200, 85, device=optimizer.device)
                    detections = optimizer.gpu_postprocessing(fake_output)
            
            time.sleep(0.1)  # æ¨¡æ‹Ÿå®é™…ä½¿ç”¨é—´éš”
        
        # æ‰“å°ä¼˜åŒ–æŠ¥å‘Š
        optimizer.print_optimization_report()
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    finally:
        optimizer.cleanup()

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¯ GPUåˆ©ç”¨ç‡ä¼˜åŒ–å™¨")
    print("="*50)
    
    print("å½“å‰é—®é¢˜:")
    print("  â€¢ GPUä½¿ç”¨ç‡: 21% (ä¸¥é‡ä¸è¶³)")
    print("  â€¢ ç³»ç»Ÿå†…å­˜: 87.7% (æ¥è¿‘ç“¶é¢ˆ)")
    print("  â€¢ GPUæ˜¾å­˜: 39.2% (å……è¶³ç©ºé—´)")
    
    print("\nä¼˜åŒ–ç­–ç•¥:")
    print("  1. å°†å±å¹•æˆªå›¾è¿ç§»åˆ°GPU")
    print("  2. åœ¨GPUä¸Šè¿›è¡Œå›¾åƒé¢„å¤„ç†")
    print("  3. GPUä¸Šè¿›è¡Œåå¤„ç†")
    print("  4. ä½¿ç”¨GPUå†…å­˜æ± å‡å°‘åˆ†é…å¼€é”€")
    print("  5. å¼‚æ­¥GPUæµæ°´çº¿å¤„ç†")
    
    print(f"\næ˜¯å¦è¿è¡ŒGPUä¼˜åŒ–æµ‹è¯•ï¼Ÿ(y/n): ", end="")
    choice = input().lower().strip()
    
    if choice == 'y':
        test_gpu_optimization()
    else:
        print("ğŸ’¡ æç¤º: åœ¨main_onnx.pyä¸­é›†æˆGPUUtilizationOptimizerç±»ä»¥è·å¾—æœ€ä½³æ•ˆæœ")

if __name__ == "__main__":
    main()