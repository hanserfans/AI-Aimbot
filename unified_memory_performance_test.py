#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€å†…å­˜æ€§èƒ½å¯¹æ¯”æµ‹è¯•
å¯¹æ¯”ä¼ ç»ŸGPUå†…å­˜ç®¡ç†ä¸CUDAç»Ÿä¸€å†…å­˜çš„æ€§èƒ½å·®å¼‚
æµ‹è¯•å†…å­˜ä½¿ç”¨æ•ˆç‡ã€å¤„ç†é€Ÿåº¦å’Œç³»ç»Ÿç¨³å®šæ€§
"""

import numpy as np
import cv2
import time
import gc
import psutil
import os
import sys
from typing import Dict, List, Tuple, Any
import torch
import matplotlib.pyplot as plt
import pandas as pd
from datetime import datetime

# å¯¼å…¥æµ‹è¯•æ¨¡å—
try:
    from gpu_accelerated_processor import get_gpu_processor
    from unified_memory_gpu_processor import get_unified_gpu_processor
    from gpu_memory_manager import get_gpu_memory_manager
    GPU_MODULES_AVAILABLE = True
except ImportError as e:
    print(f"[WARNING] GPUæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    GPU_MODULES_AVAILABLE = False

class UnifiedMemoryPerformanceTest:
    """ç»Ÿä¸€å†…å­˜æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self, test_duration_minutes: int = 5, save_results: bool = True):
        """
        åˆå§‹åŒ–æ€§èƒ½æµ‹è¯•å™¨
        
        Args:
            test_duration_minutes: æµ‹è¯•æŒç»­æ—¶é—´(åˆ†é’Ÿ)
            save_results: æ˜¯å¦ä¿å­˜æµ‹è¯•ç»“æœ
        """
        self.test_duration = test_duration_minutes * 60  # è½¬æ¢ä¸ºç§’
        self.save_results = save_results
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.results = {
            'traditional_gpu': {
                'processing_times': [],
                'memory_usage': [],
                'gpu_memory_usage': [],
                'cpu_usage': [],
                'errors': 0,
                'total_processed': 0
            },
            'unified_memory': {
                'processing_times': [],
                'memory_usage': [],
                'gpu_memory_usage': [],
                'cpu_usage': [],
                'errors': 0,
                'total_processed': 0
            }
        }
        
        # æµ‹è¯•é…ç½®
        self.test_configs = [
            # (image_size, batch_size, description)
            ((320, 320), 1, "å°å›¾åƒå•å¼ å¤„ç†"),
            ((640, 640), 1, "ä¸­ç­‰å›¾åƒå•å¼ å¤„ç†"),
            ((1920, 1080), 1, "å¤§å›¾åƒå•å¼ å¤„ç†"),
            ((320, 320), 4, "å°å›¾åƒæ‰¹å¤„ç†"),
            ((640, 640), 2, "ä¸­ç­‰å›¾åƒæ‰¹å¤„ç†"),
        ]
        
        print(f"[INFO] ğŸ§ª ç»Ÿä¸€å†…å­˜æ€§èƒ½æµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"[INFO] æµ‹è¯•æ—¶é•¿: {test_duration_minutes}åˆ†é’Ÿ")
        print(f"[INFO] æµ‹è¯•é…ç½®: {len(self.test_configs)}ç§åœºæ™¯")
        
    def generate_test_images(self, size: Tuple[int, int], batch_size: int) -> List[np.ndarray]:
        """ç”Ÿæˆæµ‹è¯•å›¾åƒ"""
        images = []
        for i in range(batch_size):
            # ç”Ÿæˆéšæœºå›¾åƒï¼Œæ¨¡æ‹ŸçœŸå®åœºæ™¯
            img = np.random.randint(0, 255, (size[1], size[0], 3), dtype=np.uint8)
            
            # æ·»åŠ ä¸€äº›å›¾åƒç‰¹å¾ï¼Œæ¨¡æ‹Ÿç›®æ ‡æ£€æµ‹åœºæ™¯
            # æ·»åŠ çŸ©å½¢æ¡†
            cv2.rectangle(img, (50, 50), (size[0]-50, size[1]-50), (255, 0, 0), 2)
            # æ·»åŠ åœ†å½¢
            cv2.circle(img, (size[0]//2, size[1]//2), min(size)//4, (0, 255, 0), -1)
            
            images.append(img)
            
        return images
        
    def get_system_metrics(self) -> Dict[str, float]:
        """è·å–ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡"""
        try:
            # CPUä½¿ç”¨ç‡
            cpu_percent = psutil.cpu_percent(interval=0.1)
            
            # å†…å­˜ä½¿ç”¨æƒ…å†µ
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            memory_used_gb = memory.used / (1024**3)
            
            # GPUå†…å­˜ä½¿ç”¨æƒ…å†µ
            gpu_memory_used = 0
            gpu_memory_total = 0
            
            if torch.cuda.is_available():
                gpu_memory_used = torch.cuda.memory_allocated() / (1024**3)
                gpu_memory_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                
            return {
                'cpu_percent': cpu_percent,
                'memory_percent': memory_percent,
                'memory_used_gb': memory_used_gb,
                'gpu_memory_used_gb': gpu_memory_used,
                'gpu_memory_total_gb': gpu_memory_total,
                'gpu_memory_percent': (gpu_memory_used / gpu_memory_total * 100) if gpu_memory_total > 0 else 0
            }
            
        except Exception as e:
            print(f"[WARNING] è·å–ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
            return {
                'cpu_percent': 0,
                'memory_percent': 0,
                'memory_used_gb': 0,
                'gpu_memory_used_gb': 0,
                'gpu_memory_total_gb': 0,
                'gpu_memory_percent': 0
            }
            
    def test_traditional_gpu_processing(self, images: List[np.ndarray], 
                                      target_size: Tuple[int, int]) -> Dict[str, Any]:
        """æµ‹è¯•ä¼ ç»ŸGPUå¤„ç†æ€§èƒ½"""
        try:
            # è·å–ä¼ ç»ŸGPUå¤„ç†å™¨
            gpu_processor = get_gpu_processor(device_id=0)
            
            start_time = time.time()
            processed_images = []
            
            # å¤„ç†æ‰€æœ‰å›¾åƒ
            for img in images:
                processed = gpu_processor.preprocess_image_gpu(img, target_size)
                processed_images.append(processed)
                
            processing_time = time.time() - start_time
            
            # è·å–ç³»ç»ŸæŒ‡æ ‡
            metrics = self.get_system_metrics()
            
            return {
                'processing_time': processing_time,
                'processed_count': len(images),
                'avg_time_per_image': processing_time / len(images),
                'metrics': metrics,
                'success': True
            }
            
        except Exception as e:
            print(f"[ERROR] ä¼ ç»ŸGPUå¤„ç†æµ‹è¯•å¤±è´¥: {e}")
            return {
                'processing_time': float('inf'),
                'processed_count': 0,
                'avg_time_per_image': float('inf'),
                'metrics': self.get_system_metrics(),
                'success': False,
                'error': str(e)
            }
            
    def test_unified_memory_processing(self, images: List[np.ndarray], 
                                     target_size: Tuple[int, int],
                                     access_pattern: str = 'mixed') -> Dict[str, Any]:
        """æµ‹è¯•ç»Ÿä¸€å†…å­˜å¤„ç†æ€§èƒ½"""
        try:
            # è·å–ç»Ÿä¸€å†…å­˜GPUå¤„ç†å™¨
            unified_processor = get_unified_gpu_processor(device_id=0, unified_memory_size_gb=2.0)
            
            start_time = time.time()
            processed_images = []
            
            # å¤„ç†æ‰€æœ‰å›¾åƒ
            for img in images:
                processed = unified_processor.preprocess_image_unified(
                    img, target_size, access_pattern=access_pattern
                )
                processed_images.append(processed)
                
            processing_time = time.time() - start_time
            
            # è·å–ç³»ç»ŸæŒ‡æ ‡
            metrics = self.get_system_metrics()
            
            # è·å–ç»Ÿä¸€å†…å­˜ç‰¹å®šæŒ‡æ ‡
            unified_stats = unified_processor.get_unified_memory_stats()
            
            return {
                'processing_time': processing_time,
                'processed_count': len(images),
                'avg_time_per_image': processing_time / len(images),
                'metrics': metrics,
                'unified_stats': unified_stats,
                'success': True
            }
            
        except Exception as e:
            print(f"[ERROR] ç»Ÿä¸€å†…å­˜å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
            return {
                'processing_time': float('inf'),
                'processed_count': 0,
                'avg_time_per_image': float('inf'),
                'metrics': self.get_system_metrics(),
                'unified_stats': {},
                'success': False,
                'error': str(e)
            }
            
    def run_single_test_scenario(self, config: Tuple) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªæµ‹è¯•åœºæ™¯"""
        image_size, batch_size, description = config
        
        print(f"\n[TEST] ğŸ”„ æµ‹è¯•åœºæ™¯: {description}")
        print(f"  å›¾åƒå°ºå¯¸: {image_size}")
        print(f"  æ‰¹å¤„ç†å¤§å°: {batch_size}")
        
        # ç”Ÿæˆæµ‹è¯•å›¾åƒ
        test_images = self.generate_test_images(image_size, batch_size)
        
        # æµ‹è¯•ä¼ ç»ŸGPUå¤„ç†
        print("  [1/3] æµ‹è¯•ä¼ ç»ŸGPUå¤„ç†...")
        traditional_result = self.test_traditional_gpu_processing(test_images, image_size)
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        time.sleep(1)  # ç­‰å¾…å†…å­˜æ¸…ç†
        
        # æµ‹è¯•ç»Ÿä¸€å†…å­˜å¤„ç† - æ··åˆæ¨¡å¼
        print("  [2/3] æµ‹è¯•ç»Ÿä¸€å†…å­˜å¤„ç†(æ··åˆæ¨¡å¼)...")
        unified_mixed_result = self.test_unified_memory_processing(test_images, image_size, 'mixed')
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        time.sleep(1)
        
        # æµ‹è¯•ç»Ÿä¸€å†…å­˜å¤„ç† - GPUé‡åº¦æ¨¡å¼
        print("  [3/3] æµ‹è¯•ç»Ÿä¸€å†…å­˜å¤„ç†(GPUé‡åº¦æ¨¡å¼)...")
        unified_gpu_result = self.test_unified_memory_processing(test_images, image_size, 'gpu_heavy')
        
        # æ¸…ç†GPUå†…å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.synchronize()
        gc.collect()
        
        return {
            'config': config,
            'traditional_gpu': traditional_result,
            'unified_mixed': unified_mixed_result,
            'unified_gpu_heavy': unified_gpu_result
        }
        
    def run_comprehensive_test(self) -> Dict[str, Any]:
        """è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•"""
        print(f"\n[INFO] ğŸš€ å¼€å§‹ç»¼åˆæ€§èƒ½æµ‹è¯•...")
        print(f"[INFO] é¢„è®¡æµ‹è¯•æ—¶é—´: {self.test_duration/60:.1f}åˆ†é’Ÿ")
        
        if not GPU_MODULES_AVAILABLE:
            print("[ERROR] GPUæ¨¡å—ä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡Œæµ‹è¯•")
            return {}
            
        test_start_time = time.time()
        scenario_results = []
        
        # è¿è¡Œæ¯ä¸ªæµ‹è¯•åœºæ™¯
        for i, config in enumerate(self.test_configs):
            print(f"\n[PROGRESS] æµ‹è¯•è¿›åº¦: {i+1}/{len(self.test_configs)}")
            
            scenario_result = self.run_single_test_scenario(config)
            scenario_results.append(scenario_result)
            
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            elapsed_time = time.time() - test_start_time
            if elapsed_time > self.test_duration:
                print(f"[INFO] â° è¾¾åˆ°æµ‹è¯•æ—¶é—´é™åˆ¶ï¼Œåœæ­¢æµ‹è¯•")
                break
                
        total_test_time = time.time() - test_start_time
        
        # æ±‡æ€»ç»“æœ
        summary = self.analyze_test_results(scenario_results, total_test_time)
        
        # ä¿å­˜ç»“æœ
        if self.save_results:
            self.save_test_results(scenario_results, summary)
            
        return {
            'scenario_results': scenario_results,
            'summary': summary,
            'total_test_time': total_test_time
        }
        
    def analyze_test_results(self, scenario_results: List[Dict], 
                           total_test_time: float) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•ç»“æœ"""
        print(f"\n[INFO] ğŸ“Š åˆ†ææµ‹è¯•ç»“æœ...")
        
        # æ”¶é›†æ€§èƒ½æ•°æ®
        traditional_times = []
        unified_mixed_times = []
        unified_gpu_times = []
        
        traditional_memory = []
        unified_mixed_memory = []
        unified_gpu_memory = []
        
        success_rates = {
            'traditional': 0,
            'unified_mixed': 0,
            'unified_gpu': 0
        }
        
        for result in scenario_results:
            # å¤„ç†æ—¶é—´
            if result['traditional_gpu']['success']:
                traditional_times.append(result['traditional_gpu']['avg_time_per_image'])
                traditional_memory.append(result['traditional_gpu']['metrics']['memory_percent'])
                success_rates['traditional'] += 1
                
            if result['unified_mixed']['success']:
                unified_mixed_times.append(result['unified_mixed']['avg_time_per_image'])
                unified_mixed_memory.append(result['unified_mixed']['metrics']['memory_percent'])
                success_rates['unified_mixed'] += 1
                
            if result['unified_gpu_heavy']['success']:
                unified_gpu_times.append(result['unified_gpu_heavy']['avg_time_per_image'])
                unified_gpu_memory.append(result['unified_gpu_heavy']['metrics']['memory_percent'])
                success_rates['unified_gpu'] += 1
                
        # è®¡ç®—å¹³å‡æ€§èƒ½
        def safe_mean(data):
            return np.mean(data) if data else float('inf')
            
        avg_traditional_time = safe_mean(traditional_times)
        avg_unified_mixed_time = safe_mean(unified_mixed_times)
        avg_unified_gpu_time = safe_mean(unified_gpu_times)
        
        avg_traditional_memory = safe_mean(traditional_memory)
        avg_unified_mixed_memory = safe_mean(unified_mixed_memory)
        avg_unified_gpu_memory = safe_mean(unified_gpu_memory)
        
        # è®¡ç®—æ€§èƒ½æå‡
        def calculate_improvement(baseline, optimized):
            if baseline == 0 or baseline == float('inf') or optimized == float('inf'):
                return 0
            return ((baseline - optimized) / baseline) * 100
            
        mixed_speed_improvement = calculate_improvement(avg_traditional_time, avg_unified_mixed_time)
        gpu_speed_improvement = calculate_improvement(avg_traditional_time, avg_unified_gpu_time)
        
        mixed_memory_improvement = calculate_improvement(avg_traditional_memory, avg_unified_mixed_memory)
        gpu_memory_improvement = calculate_improvement(avg_traditional_memory, avg_unified_gpu_memory)
        
        summary = {
            'test_scenarios': len(scenario_results),
            'total_test_time_minutes': total_test_time / 60,
            
            # å¹³å‡å¤„ç†æ—¶é—´ (æ¯«ç§’)
            'avg_processing_time_ms': {
                'traditional_gpu': avg_traditional_time * 1000,
                'unified_mixed': avg_unified_mixed_time * 1000,
                'unified_gpu_heavy': avg_unified_gpu_time * 1000
            },
            
            # å¹³å‡å†…å­˜ä½¿ç”¨ç‡ (%)
            'avg_memory_usage_percent': {
                'traditional_gpu': avg_traditional_memory,
                'unified_mixed': avg_unified_mixed_memory,
                'unified_gpu_heavy': avg_unified_gpu_memory
            },
            
            # æ€§èƒ½æå‡ (%)
            'performance_improvement_percent': {
                'unified_mixed_speed': mixed_speed_improvement,
                'unified_gpu_speed': gpu_speed_improvement,
                'unified_mixed_memory': mixed_memory_improvement,
                'unified_gpu_memory': gpu_memory_improvement
            },
            
            # æˆåŠŸç‡
            'success_rates': {
                'traditional_gpu': success_rates['traditional'] / len(scenario_results) * 100,
                'unified_mixed': success_rates['unified_mixed'] / len(scenario_results) * 100,
                'unified_gpu_heavy': success_rates['unified_gpu'] / len(scenario_results) * 100
            }
        }
        
        return summary
        
    def print_test_summary(self, summary: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print(f"\n" + "="*60)
        print(f"ğŸ“Š ç»Ÿä¸€å†…å­˜æ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print(f"="*60)
        
        print(f"\nğŸ” æµ‹è¯•æ¦‚å†µ:")
        print(f"  æµ‹è¯•åœºæ™¯æ•°é‡: {summary['test_scenarios']}")
        print(f"  æ€»æµ‹è¯•æ—¶é—´: {summary['total_test_time_minutes']:.1f}åˆ†é’Ÿ")
        
        print(f"\nâš¡ å¹³å‡å¤„ç†æ—¶é—´ (æ¯å¼ å›¾åƒ):")
        times = summary['avg_processing_time_ms']
        print(f"  ä¼ ç»ŸGPUå¤„ç†: {times['traditional_gpu']:.2f}ms")
        print(f"  ç»Ÿä¸€å†…å­˜(æ··åˆ): {times['unified_mixed']:.2f}ms")
        print(f"  ç»Ÿä¸€å†…å­˜(GPUé‡åº¦): {times['unified_gpu_heavy']:.2f}ms")
        
        print(f"\nğŸ’¾ å¹³å‡å†…å­˜ä½¿ç”¨ç‡:")
        memory = summary['avg_memory_usage_percent']
        print(f"  ä¼ ç»ŸGPUå¤„ç†: {memory['traditional_gpu']:.1f}%")
        print(f"  ç»Ÿä¸€å†…å­˜(æ··åˆ): {memory['unified_mixed']:.1f}%")
        print(f"  ç»Ÿä¸€å†…å­˜(GPUé‡åº¦): {memory['unified_gpu_heavy']:.1f}%")
        
        print(f"\nğŸ“ˆ æ€§èƒ½æå‡:")
        improvements = summary['performance_improvement_percent']
        print(f"  ç»Ÿä¸€å†…å­˜(æ··åˆ)é€Ÿåº¦æå‡: {improvements['unified_mixed_speed']:+.1f}%")
        print(f"  ç»Ÿä¸€å†…å­˜(GPUé‡åº¦)é€Ÿåº¦æå‡: {improvements['unified_gpu_speed']:+.1f}%")
        print(f"  ç»Ÿä¸€å†…å­˜(æ··åˆ)å†…å­˜ä¼˜åŒ–: {improvements['unified_mixed_memory']:+.1f}%")
        print(f"  ç»Ÿä¸€å†…å­˜(GPUé‡åº¦)å†…å­˜ä¼˜åŒ–: {improvements['unified_gpu_memory']:+.1f}%")
        
        print(f"\nâœ… æˆåŠŸç‡:")
        success = summary['success_rates']
        print(f"  ä¼ ç»ŸGPUå¤„ç†: {success['traditional_gpu']:.1f}%")
        print(f"  ç»Ÿä¸€å†…å­˜(æ··åˆ): {success['unified_mixed']:.1f}%")
        print(f"  ç»Ÿä¸€å†…å­˜(GPUé‡åº¦): {success['unified_gpu_heavy']:.1f}%")
        
        # æ¨èå»ºè®®
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        
        best_speed = max(improvements['unified_mixed_speed'], improvements['unified_gpu_speed'])
        best_memory = max(improvements['unified_mixed_memory'], improvements['unified_gpu_memory'])
        
        if best_speed > 10:
            print(f"  âœ… ç»Ÿä¸€å†…å­˜æ˜¾è‘—æå‡å¤„ç†é€Ÿåº¦ ({best_speed:.1f}%)")
        elif best_speed > 0:
            print(f"  âš ï¸ ç»Ÿä¸€å†…å­˜è½»å¾®æå‡å¤„ç†é€Ÿåº¦ ({best_speed:.1f}%)")
        else:
            print(f"  âŒ ç»Ÿä¸€å†…å­˜æœªèƒ½æå‡å¤„ç†é€Ÿåº¦")
            
        if best_memory > 5:
            print(f"  âœ… ç»Ÿä¸€å†…å­˜æ˜¾è‘—ä¼˜åŒ–å†…å­˜ä½¿ç”¨ ({best_memory:.1f}%)")
        elif best_memory > 0:
            print(f"  âš ï¸ ç»Ÿä¸€å†…å­˜è½»å¾®ä¼˜åŒ–å†…å­˜ä½¿ç”¨ ({best_memory:.1f}%)")
        else:
            print(f"  âŒ ç»Ÿä¸€å†…å­˜æœªèƒ½ä¼˜åŒ–å†…å­˜ä½¿ç”¨")
            
        if improvements['unified_gpu_speed'] > improvements['unified_mixed_speed']:
            print(f"  ğŸ¯ æ¨èä½¿ç”¨GPUé‡åº¦è®¿é—®æ¨¡å¼")
        else:
            print(f"  ğŸ¯ æ¨èä½¿ç”¨æ··åˆè®¿é—®æ¨¡å¼")
            
        print(f"\n" + "="*60)
        
    def save_test_results(self, scenario_results: List[Dict], summary: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            
            # ä¿å­˜è¯¦ç»†ç»“æœ
            results_file = f"unified_memory_test_results_{timestamp}.json"
            import json
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'scenario_results': scenario_results,
                    'summary': summary,
                    'timestamp': timestamp
                }, f, indent=2, ensure_ascii=False, default=str)
                
            print(f"[INFO] ğŸ’¾ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
            
            # ç”Ÿæˆæ€§èƒ½å›¾è¡¨
            self.generate_performance_charts(scenario_results, summary, timestamp)
            
        except Exception as e:
            print(f"[WARNING] ä¿å­˜æµ‹è¯•ç»“æœå¤±è´¥: {e}")
            
    def generate_performance_charts(self, scenario_results: List[Dict], 
                                  summary: Dict[str, Any], timestamp: str):
        """ç”Ÿæˆæ€§èƒ½å¯¹æ¯”å›¾è¡¨"""
        try:
            import matplotlib.pyplot as plt
            plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS']
            plt.rcParams['axes.unicode_minus'] = False
            
            fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
            
            # å›¾1: å¤„ç†æ—¶é—´å¯¹æ¯”
            methods = ['ä¼ ç»ŸGPU', 'ç»Ÿä¸€å†…å­˜(æ··åˆ)', 'ç»Ÿä¸€å†…å­˜(GPUé‡åº¦)']
            times = [
                summary['avg_processing_time_ms']['traditional_gpu'],
                summary['avg_processing_time_ms']['unified_mixed'],
                summary['avg_processing_time_ms']['unified_gpu_heavy']
            ]
            
            bars1 = ax1.bar(methods, times, color=['#ff7f0e', '#2ca02c', '#1f77b4'])
            ax1.set_title('å¹³å‡å¤„ç†æ—¶é—´å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax1.set_ylabel('å¤„ç†æ—¶é—´ (æ¯«ç§’)')
            ax1.grid(True, alpha=0.3)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, time in zip(bars1, times):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{time:.1f}ms', ha='center', va='bottom')
            
            # å›¾2: å†…å­˜ä½¿ç”¨ç‡å¯¹æ¯”
            memory_usage = [
                summary['avg_memory_usage_percent']['traditional_gpu'],
                summary['avg_memory_usage_percent']['unified_mixed'],
                summary['avg_memory_usage_percent']['unified_gpu_heavy']
            ]
            
            bars2 = ax2.bar(methods, memory_usage, color=['#ff7f0e', '#2ca02c', '#1f77b4'])
            ax2.set_title('å¹³å‡å†…å­˜ä½¿ç”¨ç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax2.set_ylabel('å†…å­˜ä½¿ç”¨ç‡ (%)')
            ax2.grid(True, alpha=0.3)
            
            for bar, usage in zip(bars2, memory_usage):
                height = bar.get_height()
                ax2.text(bar.get_x() + bar.get_width()/2., height + height*0.01,
                        f'{usage:.1f}%', ha='center', va='bottom')
            
            # å›¾3: æ€§èƒ½æå‡ç™¾åˆ†æ¯”
            improvements = [
                summary['performance_improvement_percent']['unified_mixed_speed'],
                summary['performance_improvement_percent']['unified_gpu_speed'],
                summary['performance_improvement_percent']['unified_mixed_memory'],
                summary['performance_improvement_percent']['unified_gpu_memory']
            ]
            
            improvement_labels = ['æ··åˆæ¨¡å¼\né€Ÿåº¦æå‡', 'GPUé‡åº¦\né€Ÿåº¦æå‡', 'æ··åˆæ¨¡å¼\nå†…å­˜ä¼˜åŒ–', 'GPUé‡åº¦\nå†…å­˜ä¼˜åŒ–']
            colors = ['#2ca02c' if x > 0 else '#d62728' for x in improvements]
            
            bars3 = ax3.bar(improvement_labels, improvements, color=colors)
            ax3.set_title('æ€§èƒ½æå‡ç™¾åˆ†æ¯”', fontsize=14, fontweight='bold')
            ax3.set_ylabel('æå‡ç™¾åˆ†æ¯” (%)')
            ax3.axhline(y=0, color='black', linestyle='-', alpha=0.3)
            ax3.grid(True, alpha=0.3)
            
            for bar, improvement in zip(bars3, improvements):
                height = bar.get_height()
                ax3.text(bar.get_x() + bar.get_width()/2., 
                        height + (1 if height >= 0 else -3),
                        f'{improvement:+.1f}%', ha='center', 
                        va='bottom' if height >= 0 else 'top')
            
            # å›¾4: æˆåŠŸç‡å¯¹æ¯”
            success_rates = [
                summary['success_rates']['traditional_gpu'],
                summary['success_rates']['unified_mixed'],
                summary['success_rates']['unified_gpu_heavy']
            ]
            
            bars4 = ax4.bar(methods, success_rates, color=['#ff7f0e', '#2ca02c', '#1f77b4'])
            ax4.set_title('æµ‹è¯•æˆåŠŸç‡å¯¹æ¯”', fontsize=14, fontweight='bold')
            ax4.set_ylabel('æˆåŠŸç‡ (%)')
            ax4.set_ylim(0, 105)
            ax4.grid(True, alpha=0.3)
            
            for bar, rate in zip(bars4, success_rates):
                height = bar.get_height()
                ax4.text(bar.get_x() + bar.get_width()/2., height + 1,
                        f'{rate:.1f}%', ha='center', va='bottom')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_file = f"unified_memory_performance_chart_{timestamp}.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            print(f"[INFO] ğŸ“Š æ€§èƒ½å›¾è¡¨å·²ä¿å­˜åˆ°: {chart_file}")
            
        except Exception as e:
            print(f"[WARNING] ç”Ÿæˆæ€§èƒ½å›¾è¡¨å¤±è´¥: {e}")

def main():
    """ä¸»å‡½æ•°"""
    print("[INFO] ğŸš€ å¯åŠ¨ç»Ÿä¸€å†…å­˜æ€§èƒ½æµ‹è¯•...")
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("[ERROR] CUDAä¸å¯ç”¨ï¼Œæ— æ³•è¿›è¡ŒGPUæµ‹è¯•")
        return
        
    print(f"[INFO] GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
    print(f"[INFO] GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB")
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = UnifiedMemoryPerformanceTest(test_duration_minutes=3, save_results=True)
    
    # è¿è¡Œæµ‹è¯•
    results = tester.run_comprehensive_test()
    
    if results:
        # æ‰“å°æ‘˜è¦
        tester.print_test_summary(results['summary'])
        
        print(f"\n[INFO] âœ… æ€§èƒ½æµ‹è¯•å®Œæˆ!")
        print(f"[INFO] æ€»æµ‹è¯•æ—¶é—´: {results['total_test_time']:.1f}ç§’")
    else:
        print(f"[ERROR] æ€§èƒ½æµ‹è¯•å¤±è´¥")

if __name__ == "__main__":
    main()