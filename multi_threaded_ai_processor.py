"""
å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿ
å……åˆ†åˆ©ç”¨å¤šæ ¸CPUå’ŒGPUï¼Œå®ç°å¹¶è¡ŒAIæ¨ç†å’Œåå¤„ç†
"""

import numpy as np
import torch
import onnxruntime as ort
import time
import threading
import queue
import multiprocessing as mp
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from typing import Optional, Dict, List, Tuple, Any
import psutil
import pandas as pd
from utils.general import non_max_suppression, xyxy2xywh
import cv2

class MultiThreadedAIProcessor:
    """å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿ"""
    
    def __init__(self,
                 model_path: str,
                 num_inference_threads: int = None,
                 num_postprocess_threads: int = None,
                 batch_size: int = 4,
                 enable_gpu_inference: bool = True,
                 enable_parallel_postprocess: bool = True):
        """
        åˆå§‹åŒ–å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿ
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„
            num_inference_threads: æ¨ç†çº¿ç¨‹æ•°
            num_postprocess_threads: åå¤„ç†çº¿ç¨‹æ•°
            batch_size: æ‰¹å¤„ç†å¤§å°
            enable_gpu_inference: å¯ç”¨GPUæ¨ç†
            enable_parallel_postprocess: å¯ç”¨å¹¶è¡Œåå¤„ç†
        """
        self.model_path = model_path
        self.batch_size = batch_size
        self.enable_gpu_inference = enable_gpu_inference
        self.enable_parallel_postprocess = enable_parallel_postprocess
        
        # è‡ªåŠ¨æ£€æµ‹æœ€ä¼˜çº¿ç¨‹æ•°
        cpu_count = psutil.cpu_count(logical=True)
        self.num_inference_threads = num_inference_threads or min(4, max(2, cpu_count // 4))
        self.num_postprocess_threads = num_postprocess_threads or min(8, max(4, cpu_count // 2))
        
        print(f"[INFO] ğŸ§  å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿåˆå§‹åŒ–")
        print(f"   â€¢ æ¨ç†çº¿ç¨‹æ•°: {self.num_inference_threads}")
        print(f"   â€¢ åå¤„ç†çº¿ç¨‹æ•°: {self.num_postprocess_threads}")
        print(f"   â€¢ æ‰¹å¤„ç†å¤§å°: {batch_size}")
        print(f"   â€¢ GPUæ¨ç†: {enable_gpu_inference}")
        print(f"   â€¢ å¹¶è¡Œåå¤„ç†: {enable_parallel_postprocess}")
        
        # åˆå§‹åŒ–æ¨¡å‹ä¼šè¯æ± 
        self.model_sessions = []
        self._initialize_model_sessions()
        
        # çº¿ç¨‹æ± 
        self.inference_executor = ThreadPoolExecutor(max_workers=self.num_inference_threads)
        self.postprocess_executor = ThreadPoolExecutor(max_workers=self.num_postprocess_threads)
        
        # é˜Ÿåˆ—ç³»ç»Ÿ
        self.input_queue = queue.Queue(maxsize=20)
        self.inference_queue = queue.Queue(maxsize=10)
        self.postprocess_queue = queue.Queue(maxsize=10)  # æ·»åŠ ç¼ºå¤±çš„åå¤„ç†é˜Ÿåˆ—
        self.output_queue = queue.Queue(maxsize=10)
        
        # æ‰¹å¤„ç†ç¼“å†²åŒº
        self.batch_buffer = []
        self.batch_lock = threading.Lock()
        
        # æ§åˆ¶å˜é‡
        self.running = False
        self.worker_threads = []
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'frames_received': 0,
            'frames_processed': 0,
            'inference_count': 0,
            'postprocess_count': 0,
            'avg_inference_time': 0.0,
            'avg_postprocess_time': 0.0,
            'throughput_fps': 0.0,
            'batch_efficiency': 0.0
        }
        
        print(f"[SUCCESS] âœ… å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def _initialize_model_sessions(self):
        """åˆå§‹åŒ–æ¨¡å‹ä¼šè¯æ± """
        try:
            # é…ç½®ONNX Runtime
            providers = []
            if self.enable_gpu_inference and ort.get_available_providers():
                if 'CUDAExecutionProvider' in ort.get_available_providers():
                    providers.append(('CUDAExecutionProvider', {
                        'device_id': 0,
                        'arena_extend_strategy': 'kNextPowerOfTwo',
                        'gpu_mem_limit': 2 * 1024 * 1024 * 1024,  # 2GB
                        'cudnn_conv_algo_search': 'EXHAUSTIVE',
                        'do_copy_in_default_stream': True,
                    }))
                elif 'DmlExecutionProvider' in ort.get_available_providers():
                    providers.append('DmlExecutionProvider')
            
            providers.append('CPUExecutionProvider')
            
            # åˆ›å»ºå¤šä¸ªä¼šè¯å®ä¾‹
            for i in range(self.num_inference_threads):
                session_options = ort.SessionOptions()
                session_options.inter_op_num_threads = 2
                session_options.intra_op_num_threads = 4
                session_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL
                session_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
                
                session = ort.InferenceSession(
                    self.model_path,
                    sess_options=session_options,
                    providers=providers
                )
                
                self.model_sessions.append({
                    'session': session,
                    'input_name': session.get_inputs()[0].name,
                    'output_names': [output.name for output in session.get_outputs()],
                    'lock': threading.Lock(),
                    'usage_count': 0
                })
            
            print(f"[INFO] âœ… åˆ›å»ºäº† {len(self.model_sessions)} ä¸ªæ¨¡å‹ä¼šè¯")
            
        except Exception as e:
            print(f"[ERROR] æ¨¡å‹ä¼šè¯åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    def start(self):
        """å¯åŠ¨AIå¤„ç†ç³»ç»Ÿ"""
        if self.running:
            return
        
        self.running = True
        
        print(f"[INFO] ğŸš€ å¯åŠ¨å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿ")
        
        # å¯åŠ¨æ‰¹å¤„ç†æ”¶é›†çº¿ç¨‹
        batch_thread = threading.Thread(
            target=self._batch_collector_worker,
            daemon=True,
            name="BatchCollector"
        )
        batch_thread.start()
        self.worker_threads.append(batch_thread)
        
        # å¯åŠ¨æ¨ç†å·¥ä½œçº¿ç¨‹
        for i in range(self.num_inference_threads):
            thread = threading.Thread(
                target=self._inference_worker,
                args=(i,),
                daemon=True,
                name=f"InferenceWorker-{i}"
            )
            thread.start()
            self.worker_threads.append(thread)
        
        # å¯åŠ¨åå¤„ç†å·¥ä½œçº¿ç¨‹
        for i in range(self.num_postprocess_threads):
            thread = threading.Thread(
                target=self._postprocess_worker,
                args=(i,),
                daemon=True,
                name=f"PostprocessWorker-{i}"
            )
            thread.start()
            self.worker_threads.append(thread)
        
        # å¯åŠ¨ç»Ÿè®¡çº¿ç¨‹
        stats_thread = threading.Thread(
            target=self._stats_worker,
            daemon=True,
            name="StatsWorker"
        )
        stats_thread.start()
        self.worker_threads.append(stats_thread)
        
        print(f"[SUCCESS] âœ… å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿå·²å¯åŠ¨")
    
    def stop(self):
        """åœæ­¢AIå¤„ç†ç³»ç»Ÿ"""
        if not self.running:
            return
        
        print("[INFO] ğŸ›‘ åœæ­¢å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿ...")
        self.running = False
        
        # ç­‰å¾…çº¿ç¨‹ç»“æŸ
        for thread in self.worker_threads:
            thread.join(timeout=1.0)
        
        # æ¸…ç©ºé˜Ÿåˆ—
        for q in [self.input_queue, self.inference_queue, self.output_queue]:
            while not q.empty():
                try:
                    q.get_nowait()
                except queue.Empty:
                    break
        
        print("[SUCCESS] âœ… å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿå·²åœæ­¢")
    
    def process_frame_async(self, frame: np.ndarray, metadata: Dict = None) -> bool:
        """å¼‚æ­¥å¤„ç†å¸§"""
        try:
            frame_data = {
                'frame': frame,
                'metadata': metadata or {},
                'timestamp': time.time(),
                'frame_id': self.stats['frames_received']
            }
            
            self.input_queue.put_nowait(frame_data)
            self.stats['frames_received'] += 1
            return True
            
        except queue.Full:
            return False
    
    def get_result(self, timeout: float = 0.001) -> Optional[Dict]:
        """è·å–å¤„ç†ç»“æœ"""
        try:
            return self.output_queue.get(timeout=timeout)
        except queue.Empty:
            return None
    
    def _batch_collector_worker(self):
        """æ‰¹å¤„ç†æ”¶é›†å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                # æ”¶é›†æ‰¹å¤„ç†æ•°æ®
                batch_frames = []
                batch_metadata = []
                
                # å°è¯•å¡«æ»¡æ‰¹æ¬¡
                for _ in range(self.batch_size):
                    try:
                        frame_data = self.input_queue.get(timeout=0.001)
                        batch_frames.append(frame_data['frame'])
                        batch_metadata.append(frame_data)
                    except queue.Empty:
                        break
                
                # å¦‚æœæœ‰æ•°æ®å°±å¤„ç†
                if batch_frames:
                    # å‡†å¤‡æ‰¹å¤„ç†è¾“å…¥
                    if len(batch_frames) == 1:
                        # å•å¸§å¤„ç†
                        batch_input = np.expand_dims(batch_frames[0], 0)
                    else:
                        # å¤šå¸§æ‰¹å¤„ç†
                        batch_input = np.stack(batch_frames, axis=0)
                    
                    # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ (NCHW)
                    if batch_input.ndim == 4:
                        # æ‰¹å¤„ç†æƒ…å†µ
                        if batch_input.shape[-1] == 4:
                            # ç§»é™¤alphaé€šé“ (RGBA -> RGB)
                            batch_input = batch_input[:, :, :, :3]
                        if batch_input.shape[-1] == 3:
                            # ä» (batch, H, W, C) è½¬æ¢ä¸º (batch, C, H, W)
                            batch_input = np.transpose(batch_input, (0, 3, 1, 2))
                    elif batch_input.ndim == 3:
                        # å•å¸§æƒ…å†µ
                        if batch_input.shape[-1] == 4:
                            # ç§»é™¤alphaé€šé“ (RGBA -> RGB)
                            batch_input = batch_input[:, :, :3]
                        if batch_input.shape[-1] == 3:
                            # ä» (H, W, C) è½¬æ¢ä¸º (C, H, W)
                            batch_input = np.transpose(batch_input, (2, 0, 1))
                        # æ·»åŠ æ‰¹æ¬¡ç»´åº¦ (C, H, W) -> (1, C, H, W)
                        batch_input = np.expand_dims(batch_input, axis=0)
                    
                    # ç¡®ä¿æ•°æ®ç±»å‹å’Œæ•°å€¼èŒƒå›´æ­£ç¡®
                    if batch_input.dtype == np.uint8:
                        batch_input = batch_input.astype(np.float32) / 255.0
                    elif batch_input.dtype != np.float32:
                        batch_input = batch_input.astype(np.float32)
                    
                    # è½¬æ¢ä¸ºæ¨¡å‹æœŸæœ›çš„float16ç±»å‹
                    batch_input = batch_input.astype(np.float16)
                    
                    print(f"[DEBUG] æ‰¹å¤„ç†è¾“å…¥å½¢çŠ¶: {batch_input.shape}, æ•°æ®ç±»å‹: {batch_input.dtype}")
                    
                    batch_data = {
                        'input': batch_input,  # ä½¿ç”¨float16åŒ¹é…æ¨¡å‹æœŸæœ›
                        'metadata': batch_metadata,
                        'batch_size': len(batch_frames),
                        'timestamp': time.time()
                    }
                    
                    # æ”¾å…¥æ¨ç†é˜Ÿåˆ—
                    try:
                        self.inference_queue.put_nowait(batch_data)
                    except queue.Full:
                        # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„æ‰¹æ¬¡
                        try:
                            self.inference_queue.get_nowait()
                            self.inference_queue.put_nowait(batch_data)
                        except queue.Empty:
                            pass
                
                else:
                    time.sleep(0.0001)  # 0.1mså¾®å»¶è¿Ÿ
                    
            except Exception as e:
                print(f"[ERROR] æ‰¹å¤„ç†æ”¶é›†çº¿ç¨‹é”™è¯¯: {e}")
                time.sleep(0.001)
    
    def _inference_worker(self, worker_id: int):
        """æ¨ç†å·¥ä½œçº¿ç¨‹"""
        session_info = self.model_sessions[worker_id % len(self.model_sessions)]
        
        while self.running:
            try:
                # è·å–æ‰¹å¤„ç†æ•°æ®
                batch_data = self.inference_queue.get(timeout=0.1)
                
                inference_start = time.time()
                
                # æ‰§è¡Œæ¨ç†
                with session_info['lock']:
                    session_info['usage_count'] += 1
                    
                    outputs = session_info['session'].run(
                        session_info['output_names'],
                        {session_info['input_name']: batch_data['input']}
                    )
                
                inference_time = time.time() - inference_start
                
                # å‡†å¤‡æ¨ç†ç»“æœ
                inference_result = {
                    'outputs': outputs,
                    'metadata': batch_data['metadata'],
                    'batch_size': batch_data['batch_size'],
                    'inference_time': inference_time,
                    'worker_id': worker_id,
                    'timestamp': batch_data['timestamp']
                }
                
                # æ”¾å…¥åå¤„ç†é˜Ÿåˆ—
                try:
                    self.postprocess_queue.put_nowait(inference_result)  # ä¿®å¤ï¼šåº”è¯¥æ”¾å…¥åå¤„ç†é˜Ÿåˆ—
                    self.stats['inference_count'] += 1
                except queue.Full:
                    pass
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"[ERROR] æ¨ç†çº¿ç¨‹ {worker_id} é”™è¯¯: {e}")
                time.sleep(0.001)
    
    def _postprocess_worker(self, worker_id: int):
        """åå¤„ç†å·¥ä½œçº¿ç¨‹"""
        while self.running:
            try:
                # è·å–æ¨ç†ç»“æœ
                inference_result = self.postprocess_queue.get(timeout=0.1)  # ä¿®å¤ï¼šä»åå¤„ç†é˜Ÿåˆ—è·å–æ•°æ®
                
                postprocess_start = time.time()
                
                # æ‰§è¡Œåå¤„ç†
                processed_results = self._postprocess_batch(
                    inference_result['outputs'],
                    inference_result['metadata'],
                    inference_result['batch_size']
                )
                
                postprocess_time = time.time() - postprocess_start
                
                # ä¸ºæ¯ä¸ªç»“æœåˆ›å»ºè¾“å‡º
                for i, result in enumerate(processed_results):
                    output_data = {
                        'detections': result,
                        'metadata': inference_result['metadata'][i],
                        'inference_time': inference_result['inference_time'],
                        'postprocess_time': postprocess_time,
                        'total_time': inference_result['inference_time'] + postprocess_time,
                        'worker_id': worker_id,
                        'timestamp': inference_result['timestamp']
                    }
                    
                    # æ”¾å…¥è¾“å‡ºé˜Ÿåˆ—
                    try:
                        self.output_queue.put_nowait(output_data)
                        self.stats['postprocess_count'] += 1
                        self.stats['frames_processed'] += 1
                    except queue.Full:
                        # é˜Ÿåˆ—æ»¡æ—¶ä¸¢å¼ƒæœ€æ—§çš„ç»“æœ
                        try:
                            self.output_queue.get_nowait()
                            self.output_queue.put_nowait(output_data)
                        except queue.Empty:
                            pass
                
            except queue.Empty:
                continue
            except Exception as e:
                print(f"[ERROR] åå¤„ç†çº¿ç¨‹ {worker_id} é”™è¯¯: {e}")
                time.sleep(0.001)
    
    def _postprocess_batch(self, outputs: List[np.ndarray], metadata: List[Dict], batch_size: int) -> List[pd.DataFrame]:
        """æ‰¹å¤„ç†åå¤„ç†"""
        results = []
        
        try:
            # æ£€æŸ¥outputsæ˜¯å¦æœ‰æ•ˆ
            if not outputs or len(outputs) == 0:
                print("[WARNING] åå¤„ç†æ”¶åˆ°ç©ºçš„outputs")
                for _ in range(batch_size):
                    results.append(pd.DataFrame(columns=['x1', 'y1', 'x2', 'y2', 'confidence', 'class', 'center_x', 'center_y', 'width', 'height']))
                return results
            
            # å‡è®¾è¾“å‡ºæ˜¯ [batch_size, num_detections, 6] æ ¼å¼
            predictions = outputs[0]  # ä¸»è¦è¾“å‡º
            
            for i in range(batch_size):
                # æå–å•ä¸ªæ ·æœ¬çš„é¢„æµ‹
                pred = predictions[i:i+1]  # ä¿æŒæ‰¹æ¬¡ç»´åº¦
                
                # åº”ç”¨NMS
                pred_nms = non_max_suppression(
                    torch.from_numpy(pred),
                    conf_thres=0.45,
                    iou_thres=0.45,
                    classes=None,
                    agnostic=False,
                    max_det=10
                )
                
                # è½¬æ¢ä¸ºDataFrame
                if pred_nms[0] is not None and len(pred_nms[0]) > 0:
                    detections = pred_nms[0].cpu().numpy()
                    
                    df = pd.DataFrame(detections, columns=['x1', 'y1', 'x2', 'y2', 'confidence', 'class'])
                    
                    # è®¡ç®—ä¸­å¿ƒç‚¹å’Œå°ºå¯¸
                    df['center_x'] = (df['x1'] + df['x2']) / 2
                    df['center_y'] = (df['y1'] + df['y2']) / 2
                    df['width'] = df['x2'] - df['x1']
                    df['height'] = df['y2'] - df['y1']
                    
                    results.append(df)
                else:
                    # ç©ºæ£€æµ‹ç»“æœ
                    results.append(pd.DataFrame(columns=['x1', 'y1', 'x2', 'y2', 'confidence', 'class', 'center_x', 'center_y', 'width', 'height']))
            
        except Exception as e:
            print(f"[ERROR] æ‰¹å¤„ç†åå¤„ç†å¤±è´¥: {e}")
            # è¿”å›ç©ºç»“æœ
            for _ in range(batch_size):
                results.append(pd.DataFrame(columns=['x1', 'y1', 'x2', 'y2', 'confidence', 'class', 'center_x', 'center_y', 'width', 'height']))
        
        return results
    
    def _stats_worker(self):
        """ç»Ÿè®¡å·¥ä½œçº¿ç¨‹"""
        last_stats_time = time.time()
        last_processed = 0
        
        while self.running:
            try:
                time.sleep(1.0)  # æ¯ç§’æ›´æ–°ç»Ÿè®¡
                
                current_time = time.time()
                time_diff = current_time - last_stats_time
                
                # è®¡ç®—ååé‡
                processed_diff = self.stats['frames_processed'] - last_processed
                self.stats['throughput_fps'] = processed_diff / time_diff
                
                # è®¡ç®—æ‰¹å¤„ç†æ•ˆç‡
                if self.stats['inference_count'] > 0:
                    self.stats['batch_efficiency'] = self.stats['frames_processed'] / (self.stats['inference_count'] * self.batch_size)
                
                # æ›´æ–°è®°å½•
                last_stats_time = current_time
                last_processed = self.stats['frames_processed']
                
            except Exception as e:
                print(f"[ERROR] ç»Ÿè®¡çº¿ç¨‹é”™è¯¯: {e}")
    
    def get_performance_stats(self) -> Dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = self.stats.copy()
        
        # æ·»åŠ é˜Ÿåˆ—çŠ¶æ€
        stats['queue_sizes'] = {
            'input': self.input_queue.qsize(),
            'inference': self.inference_queue.qsize(),
            'output': self.output_queue.qsize()
        }
        
        # æ·»åŠ ä¼šè¯ä½¿ç”¨æƒ…å†µ
        stats['session_usage'] = [session['usage_count'] for session in self.model_sessions]
        
        return stats
    
    def print_performance_stats(self):
        """æ‰“å°æ€§èƒ½ç»Ÿè®¡"""
        stats = self.get_performance_stats()
        print(f"\nğŸ§  å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿç»Ÿè®¡:")
        print(f"   â€¢ ååé‡FPS: {stats['throughput_fps']:.1f}")
        print(f"   â€¢ æ€»æ¥æ”¶å¸§æ•°: {stats['frames_received']}")
        print(f"   â€¢ æ€»å¤„ç†å¸§æ•°: {stats['frames_processed']}")
        print(f"   â€¢ æ¨ç†æ¬¡æ•°: {stats['inference_count']}")
        print(f"   â€¢ åå¤„ç†æ¬¡æ•°: {stats['postprocess_count']}")
        print(f"   â€¢ æ‰¹å¤„ç†æ•ˆç‡: {stats['batch_efficiency']:.2%}")
        print(f"   â€¢ é˜Ÿåˆ—å¤§å°: {stats['queue_sizes']}")
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.stop()
        
        if hasattr(self, 'inference_executor'):
            self.inference_executor.shutdown(wait=False)
        
        if hasattr(self, 'postprocess_executor'):
            self.postprocess_executor.shutdown(wait=False)
        
        # æ¸…ç†æ¨¡å‹ä¼šè¯
        for session_info in self.model_sessions:
            try:
                del session_info['session']
            except:
                pass
        
        print("[INFO] âœ… å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿèµ„æºå·²æ¸…ç†")

def create_multi_threaded_ai_processor(model_path: str, **kwargs):
    """åˆ›å»ºå¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿ"""
    return MultiThreadedAIProcessor(model_path=model_path, **kwargs)

if __name__ == "__main__":
    # æµ‹è¯•å¤šçº¿ç¨‹AIå¤„ç†ç³»ç»Ÿ
    processor = create_multi_threaded_ai_processor(
        model_path="yolov5s320Half.onnx",
        batch_size=4,
        num_inference_threads=2,
        num_postprocess_threads=4
    )
    
    try:
        processor.start()
        
        print("[INFO] æµ‹è¯•è¿è¡Œ30ç§’...")
        start_time = time.time()
        
        # æ¨¡æ‹Ÿè¾“å…¥å¸§
        test_frame = np.random.randint(0, 255, (320, 320, 3), dtype=np.uint8).astype(np.float16) / 255.0
        
        while time.time() - start_time < 30:
            # æäº¤å¤„ç†ä»»åŠ¡
            processor.process_frame_async(test_frame)
            
            # è·å–ç»“æœ
            result = processor.get_result()
            if result:
                print(f"å¤„ç†å®Œæˆ: {result['timestamp']:.3f}, æ£€æµ‹æ•°: {len(result['detections'])}")
            
            time.sleep(0.01)  # 100FPSè¾“å…¥
        
        processor.print_performance_stats()
        
    finally:
        processor.cleanup()