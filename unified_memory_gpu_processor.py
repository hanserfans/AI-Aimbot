#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ç»Ÿä¸€å†…å­˜GPUåŠ é€Ÿå¤„ç†å™¨
é›†æˆCUDAç»Ÿä¸€å†…å­˜ç®¡ç†ï¼Œå®ç°é«˜æ•ˆçš„CPU-GPUååŒå¤„ç†
è‡ªåŠ¨å†…å­˜è¿ç§»ï¼Œä¼˜åŒ–å›¾åƒå¤„ç†æ€§èƒ½
"""

import numpy as np
import cv2
import time
import gc
from typing import Tuple, Optional, Any, Dict
import torch
import torch.nn.functional as F
from cuda_unified_memory_manager import get_unified_memory_manager, CUDAUnifiedMemoryManager

try:
    import cupy as cp
    CUPY_AVAILABLE = True
    print("[INFO] âœ… CuPyå¯ç”¨ï¼Œå¯ç”¨CUDAåŠ é€Ÿ")
except ImportError:
    CUPY_AVAILABLE = False
    print("[WARNING] CuPyä¸å¯ç”¨ï¼Œä½¿ç”¨PyTorch CUDA")

class UnifiedMemoryGPUProcessor:
    """ç»Ÿä¸€å†…å­˜GPUåŠ é€Ÿå¤„ç†å™¨ - é›†æˆCUDAç»Ÿä¸€å†…å­˜ç®¡ç†"""
    
    def __init__(self, device_id: int = 0, unified_memory_size_gb: float = 2.0, 
                 enable_auto_migration: bool = True):
        """
        åˆå§‹åŒ–ç»Ÿä¸€å†…å­˜GPUå¤„ç†å™¨
        
        Args:
            device_id: GPUè®¾å¤‡ID
            unified_memory_size_gb: ç»Ÿä¸€å†…å­˜æ± å¤§å°(GB)
            enable_auto_migration: æ˜¯å¦å¯ç”¨è‡ªåŠ¨å†…å­˜è¿ç§»
        """
        self.device_id = device_id
        self.device = f'cuda:{device_id}' if torch.cuda.is_available() else 'cpu'
        self.enable_auto_migration = enable_auto_migration
        
        # åˆå§‹åŒ–ç»Ÿä¸€å†…å­˜ç®¡ç†å™¨
        self.unified_memory_manager = get_unified_memory_manager([device_id], unified_memory_size_gb)
        
        # é¢„åˆ†é…ç»Ÿä¸€å†…å­˜ç¼“å†²åŒº
        self.unified_buffers = {}
        self.buffer_access_patterns = {}
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'gpu_preprocessing_time': [],
            'gpu_postprocessing_time': [],
            'unified_memory_hits': 0,
            'unified_memory_misses': 0,
            'auto_migrations': 0,
            'manual_migrations': 0,
            'total_processing_time': 0.0,
            'memory_efficiency_score': 0.0
        }
        
        # é¢„åˆ†é…å¸¸ç”¨çš„ç»Ÿä¸€å†…å­˜ç¼“å†²åŒº
        self._preallocate_unified_memory()
        
        print(f"[INFO] ğŸŒ ç»Ÿä¸€å†…å­˜GPUå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"[INFO] è®¾å¤‡: {self.device}")
        print(f"[INFO] ç»Ÿä¸€å†…å­˜: {unified_memory_size_gb:.1f}GB")
        print(f"[INFO] è‡ªåŠ¨è¿ç§»: {'å¯ç”¨' if enable_auto_migration else 'ç¦ç”¨'}")
        
    def _preallocate_unified_memory(self):
        """é¢„åˆ†é…ç»Ÿä¸€å†…å­˜ç¼“å†²åŒº"""
        try:
            # é¢„åˆ†é…å¸¸ç”¨å°ºå¯¸çš„ç»Ÿä¸€å†…å­˜
            buffer_configs = [
                # (shape, access_pattern, description)
                ((320, 320, 3), 'mixed', 'AIæ£€æµ‹å›¾åƒ'),
                ((640, 640, 3), 'gpu_heavy', 'é«˜åˆ†è¾¨ç‡æ£€æµ‹'),
                ((1920, 1080, 3), 'cpu_heavy', 'å…¨å±æˆªå›¾'),
                ((1, 3, 320, 320), 'gpu_heavy', 'NCHWæ ¼å¼æ£€æµ‹'),
                ((1, 3, 640, 640), 'gpu_heavy', 'é«˜åˆ†è¾¨ç‡NCHW'),
                ((100, 6), 'mixed', 'æ£€æµ‹ç»“æœ'),
                ((1000, 6), 'mixed', 'å¤§æ‰¹é‡æ£€æµ‹ç»“æœ'),
            ]
            
            for shape, access_pattern, description in buffer_configs:
                key = f"unified_buffer_{shape}"
                buffer = self.unified_memory_manager.allocate_unified_memory(
                    shape, dtype=torch.float16, device=self.device, access_pattern=access_pattern
                )
                self.unified_buffers[key] = buffer
                self.buffer_access_patterns[key] = access_pattern
                
            print(f"[INFO] ğŸŒ é¢„åˆ†é…äº†{len(buffer_configs)}ä¸ªç»Ÿä¸€å†…å­˜ç¼“å†²åŒº")
            
        except Exception as e:
            print(f"[WARNING] ç»Ÿä¸€å†…å­˜é¢„åˆ†é…å¤±è´¥: {e}")
            
    def get_unified_buffer(self, shape: Tuple, access_pattern: str = 'mixed', 
                          dtype=torch.float16) -> torch.Tensor:
        """
        è·å–ç»Ÿä¸€å†…å­˜ç¼“å†²åŒº
        
        Args:
            shape: å¼ é‡å½¢çŠ¶
            access_pattern: è®¿é—®æ¨¡å¼ ('cpu_heavy', 'gpu_heavy', 'mixed')
            dtype: æ•°æ®ç±»å‹
            
        Returns:
            ç»Ÿä¸€å†…å­˜å¼ é‡
        """
        key = f"unified_buffer_{shape}"
        
        # å°è¯•ä»é¢„åˆ†é…çš„ç¼“å†²åŒºè·å–
        if key in self.unified_buffers:
            self.stats['unified_memory_hits'] += 1
            return self.unified_buffers[key]
        
        # åŠ¨æ€åˆ†é…æ–°çš„ç»Ÿä¸€å†…å­˜
        try:
            buffer = self.unified_memory_manager.allocate_unified_memory(
                shape, dtype=dtype, device=self.device, access_pattern=access_pattern
            )
            self.stats['unified_memory_misses'] += 1
            return buffer
            
        except Exception as e:
            print(f"[WARNING] ç»Ÿä¸€å†…å­˜åˆ†é…å¤±è´¥: {e}")
            # å›é€€åˆ°ä¼ ç»ŸGPUå†…å­˜
            return torch.empty(shape, dtype=dtype, device=self.device)
            
    def preprocess_image_unified(self, img: np.ndarray, target_size: Tuple[int, int] = (320, 320),
                               access_pattern: str = 'gpu_heavy') -> torch.Tensor:
        """
        ä½¿ç”¨ç»Ÿä¸€å†…å­˜è¿›è¡Œå›¾åƒé¢„å¤„ç†
        
        Args:
            img: è¾“å…¥å›¾åƒ (numpyæ•°ç»„)
            target_size: ç›®æ ‡å°ºå¯¸
            access_pattern: è®¿é—®æ¨¡å¼
            
        Returns:
            é¢„å¤„ç†åçš„GPUå¼ é‡
        """
        start_time = time.time()
        
        try:
            # æ­¥éª¤1: å°†å›¾åƒæ•°æ®åŠ è½½åˆ°ç»Ÿä¸€å†…å­˜
            if access_pattern == 'cpu_heavy':
                # CPUé‡åº¦è®¿é—®ï¼Œä¼˜å…ˆåœ¨CPUå¤„ç†
                processed_img = self._preprocess_cpu_optimized(img, target_size)
                # ç„¶åè¿ç§»åˆ°GPUï¼ˆå¦‚æœéœ€è¦ï¼‰
                if self.enable_auto_migration:
                    processed_img = self.unified_memory_manager.migrate_to_device(
                        processed_img, self.device, async_migration=True
                    )
                    self.stats['auto_migrations'] += 1
                    
            elif access_pattern == 'gpu_heavy':
                # GPUé‡åº¦è®¿é—®ï¼Œç›´æ¥åœ¨GPUå¤„ç†
                processed_img = self._preprocess_gpu_optimized(img, target_size)
                
            else:  # mixed
                # æ··åˆè®¿é—®ï¼Œä½¿ç”¨æœ€ä¼˜ç­–ç•¥
                if img.size > 1920 * 1080 * 3:  # å¤§å›¾åƒç”¨CPUé¢„å¤„ç†
                    processed_img = self._preprocess_cpu_optimized(img, target_size)
                    if self.enable_auto_migration:
                        processed_img = self.unified_memory_manager.migrate_to_device(
                            processed_img, self.device, async_migration=True
                        )
                        self.stats['auto_migrations'] += 1
                else:  # å°å›¾åƒç›´æ¥GPUå¤„ç†
                    processed_img = self._preprocess_gpu_optimized(img, target_size)
                    
            processing_time = time.time() - start_time
            self.stats['gpu_preprocessing_time'].append(processing_time)
            self.stats['total_processing_time'] += processing_time
            
            return processed_img
            
        except Exception as e:
            print(f"[WARNING] ç»Ÿä¸€å†…å­˜é¢„å¤„ç†å¤±è´¥: {e}")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            return self._preprocess_fallback(img, target_size)
    
    def preprocess_image_gpu(self, img: np.ndarray, target_size: Tuple[int, int] = (320, 320)) -> torch.Tensor:
        """
        GPUå›¾åƒé¢„å¤„ç†æ¥å£ï¼ˆå…¼å®¹æ€§æ–¹æ³•ï¼‰
        
        Args:
            img: è¾“å…¥å›¾åƒ (numpyæ•°ç»„)
            target_size: ç›®æ ‡å°ºå¯¸ï¼Œé»˜è®¤(320, 320)
            
        Returns:
            é¢„å¤„ç†åçš„GPUå¼ é‡
        """
        try:
            # éªŒè¯è¾“å…¥å›¾åƒ
            if img is None:
                raise ValueError("è¾“å…¥å›¾åƒä¸ºNone")
            
            if len(img.shape) != 3:
                raise ValueError(f"è¾“å…¥å›¾åƒç»´åº¦é”™è¯¯ï¼ŒæœŸæœ›3ç»´ï¼Œå®é™…{len(img.shape)}ç»´")
            
            if img.shape[2] not in [3, 4]:  # RGBæˆ–RGBA
                raise ValueError(f"è¾“å…¥å›¾åƒé€šé“æ•°é”™è¯¯ï¼ŒæœŸæœ›3æˆ–4é€šé“ï¼Œå®é™…{img.shape[2]}é€šé“")
            
            # å¦‚æœæ˜¯RGBAï¼Œè½¬æ¢ä¸ºRGB
            if img.shape[2] == 4:
                img = img[:, :, :3]
            
            print(f"[DEBUG] ç»Ÿä¸€å†…å­˜GPUé¢„å¤„ç†å¼€å§‹ï¼Œè¾“å…¥å½¢çŠ¶: {img.shape}")
            
            # ä½¿ç”¨ç»Ÿä¸€å†…å­˜é¢„å¤„ç†ï¼Œé»˜è®¤GPUé‡åº¦è®¿é—®æ¨¡å¼
            return self.preprocess_image_unified(img, target_size, access_pattern='gpu_heavy')
            
        except Exception as e:
            print(f"[ERROR] ç»Ÿä¸€å†…å­˜GPUé¢„å¤„ç†æ¥å£å¤±è´¥: {e}")
            print(f"[ERROR] è¾“å…¥å›¾åƒä¿¡æ¯: shape={img.shape if img is not None else 'None'}, dtype={img.dtype if img is not None else 'None'}")
            print(f"[ERROR] ç›®æ ‡å°ºå¯¸: {target_size}")
            raise
            
    def _preprocess_cpu_optimized(self, img: np.ndarray, target_size: Tuple[int, int]) -> torch.Tensor:
        """CPUä¼˜åŒ–çš„é¢„å¤„ç†"""
        try:
            print(f"[DEBUG] CPUä¼˜åŒ–é¢„å¤„ç†è¾“å…¥å›¾åƒå½¢çŠ¶: {img.shape}")
            
            # ä½¿ç”¨OpenCVè¿›è¡ŒCPUé¢„å¤„ç†
            resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)
            print(f"[DEBUG] CPUç¼©æ”¾åå›¾åƒå½¢çŠ¶: {resized.shape}")
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶å½’ä¸€åŒ–
            tensor = torch.from_numpy(resized).float() / 255.0
            
            # è½¬æ¢ä¸ºNCHWæ ¼å¼
            if len(tensor.shape) == 3:  # HWC -> NCHW
                tensor = tensor.permute(2, 0, 1).unsqueeze(0)
            
            print(f"[DEBUG] CPUé¢„å¤„ç†æœ€ç»ˆå¼ é‡å½¢çŠ¶: {tensor.shape}")
            
            # å°è¯•åˆ†é…ç»Ÿä¸€å†…å­˜
            try:
                # åˆ†é…ç»Ÿä¸€å†…å­˜å¼ é‡
                unified_tensor = self.unified_memory_manager.allocate_unified_memory(
                    tensor.shape, dtype=torch.float32, access_pattern='cpu_heavy'
                )
                
                # å°†æ•°æ®å¤åˆ¶åˆ°ç»Ÿä¸€å†…å­˜
                unified_tensor.copy_(tensor)
                
                return unified_tensor.half()  # è½¬æ¢ä¸ºfloat16èŠ‚çœå†…å­˜
                
            except Exception as unified_error:
                print(f"[WARNING] ç»Ÿä¸€å†…å­˜åˆ†é…å¤±è´¥ï¼Œä½¿ç”¨æ™®é€šGPUå†…å­˜: {unified_error}")
                # å›é€€åˆ°æ™®é€šGPUå†…å­˜
                return tensor.half().to(self.device)
            
        except Exception as e:
            print(f"[WARNING] CPUä¼˜åŒ–é¢„å¤„ç†å¤±è´¥: {e}")
            print(f"[ERROR] è¾“å…¥å›¾åƒå½¢çŠ¶: {img.shape if img is not None else 'None'}")
            print(f"[ERROR] ç›®æ ‡å°ºå¯¸: {target_size}")
            raise
            
    def _preprocess_gpu_optimized(self, img: np.ndarray, target_size: Tuple[int, int]) -> torch.Tensor:
        """GPUä¼˜åŒ–çš„é¢„å¤„ç†"""
        try:
            # æ–¹æ³•1: ä½¿ç”¨CuPyåŠ é€Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if CUPY_AVAILABLE:
                return self._preprocess_with_cupy_unified(img, target_size)
            
            # æ–¹æ³•2: ä½¿ç”¨PyTorch GPUåŠ é€Ÿ
            else:
                return self._preprocess_with_torch_unified(img, target_size)
                
        except Exception as e:
            print(f"[WARNING] GPUä¼˜åŒ–é¢„å¤„ç†å¤±è´¥: {e}")
            raise
            
    def _preprocess_with_cupy_unified(self, img: np.ndarray, target_size: Tuple[int, int]) -> torch.Tensor:
        """ä½¿ç”¨CuPyå’Œç»Ÿä¸€å†…å­˜çš„é¢„å¤„ç†"""
        try:
            # å°†å›¾åƒæ•°æ®ä¼ è¾“åˆ°GPU
            gpu_img = cp.asarray(img)
            
            # GPUä¸Šè¿›è¡Œresizeå’Œå½’ä¸€åŒ–
            resized = cp.array(cv2.resize(cp.asnumpy(gpu_img), target_size))
            normalized = resized.astype(cp.float32) / 255.0
            
            # è½¬æ¢ä¸ºPyTorchç»Ÿä¸€å†…å­˜å¼ é‡
            tensor = torch.as_tensor(normalized, device=self.device)
            
            # è½¬æ¢ä¸ºNCHWæ ¼å¼
            if len(tensor.shape) == 3:
                tensor = tensor.permute(2, 0, 1).unsqueeze(0)
                
            return tensor.half()
            
        except Exception as e:
            print(f"[WARNING] CuPyç»Ÿä¸€å†…å­˜é¢„å¤„ç†å¤±è´¥: {e}")
            raise
            
    def _preprocess_with_torch_unified(self, img: np.ndarray, target_size: Tuple[int, int]) -> torch.Tensor:
        """ä½¿ç”¨PyTorchå’Œç»Ÿä¸€å†…å­˜çš„é¢„å¤„ç†"""
        try:
            # è·å–ç»Ÿä¸€å†…å­˜ç¼“å†²åŒº
            buffer_shape = (target_size[1], target_size[0], 3)  # HWC
            tensor = self.get_unified_buffer(buffer_shape, 'gpu_heavy', torch.float32)
            
            # å°†numpyæ•°ç»„è½¬æ¢ä¸ºtensorå¹¶ä¼ è¾“åˆ°GPU
            img_tensor = torch.from_numpy(img).float()
            img_tensor = self.unified_memory_manager.migrate_to_device(
                img_tensor, self.device, async_migration=True
            )
            
            # ä½¿ç”¨PyTorchè¿›è¡Œresizeå’Œå½’ä¸€åŒ–
            img_tensor = img_tensor.permute(2, 0, 1).unsqueeze(0)  # HWC -> NCHW
            resized = F.interpolate(img_tensor, size=target_size, mode='bilinear', align_corners=False)
            normalized = resized / 255.0
            
            return normalized.half()
            
        except Exception as e:
            print(f"[WARNING] PyTorchç»Ÿä¸€å†…å­˜é¢„å¤„ç†å¤±è´¥: {e}")
            raise
            
    def _preprocess_fallback(self, img: np.ndarray, target_size: Tuple[int, int]) -> torch.Tensor:
        """å›é€€é¢„å¤„ç†æ–¹æ³•"""
        try:
            # ç¡®ä¿è¾“å…¥å›¾åƒå°ºå¯¸æ­£ç¡®
            print(f"[DEBUG] å›é€€é¢„å¤„ç†è¾“å…¥å›¾åƒå½¢çŠ¶: {img.shape}")
            
            # ä½¿ç”¨OpenCVè¿›è¡Œå›¾åƒç¼©æ”¾
            resized = cv2.resize(img, target_size, interpolation=cv2.INTER_LINEAR)
            print(f"[DEBUG] ç¼©æ”¾åå›¾åƒå½¢çŠ¶: {resized.shape}")
            
            # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶å½’ä¸€åŒ–
            tensor = torch.from_numpy(resized).float()
            tensor = tensor / 255.0
            
            # è½¬æ¢ä¸ºNCHWæ ¼å¼ (HWC -> NCHW)
            if len(tensor.shape) == 3:  # HWCæ ¼å¼
                tensor = tensor.permute(2, 0, 1).unsqueeze(0)  # HWC -> NCHW
            elif len(tensor.shape) == 4:  # å·²ç»æ˜¯NCHWæ ¼å¼
                pass
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å›¾åƒç»´åº¦: {tensor.shape}")
            
            print(f"[DEBUG] æœ€ç»ˆå¼ é‡å½¢çŠ¶: {tensor.shape}")
            
            # è½¬æ¢ä¸ºhalfç²¾åº¦å¹¶ç§»åŠ¨åˆ°GPU
            return tensor.half().to(self.device)
            
        except Exception as e:
            print(f"[ERROR] å›é€€é¢„å¤„ç†å¤±è´¥: {e}")
            print(f"[ERROR] è¾“å…¥å›¾åƒå½¢çŠ¶: {img.shape if img is not None else 'None'}")
            print(f"[ERROR] ç›®æ ‡å°ºå¯¸: {target_size}")
            raise
            
    def postprocess_detections_unified(self, outputs: torch.Tensor, 
                                     conf_threshold: float = 0.5,
                                     access_pattern: str = 'mixed') -> torch.Tensor:
        """
        ä½¿ç”¨ç»Ÿä¸€å†…å­˜è¿›è¡Œæ£€æµ‹åå¤„ç†
        
        Args:
            outputs: æ¨¡å‹è¾“å‡ºå¼ é‡
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            access_pattern: è®¿é—®æ¨¡å¼
            
        Returns:
            åå¤„ç†ç»“æœ
        """
        start_time = time.time()
        
        try:
            # ç¡®ä¿è¾“å‡ºåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if access_pattern == 'cpu_heavy':
                # CPUé‡åº¦è®¿é—®ï¼Œè¿ç§»åˆ°CPUå¤„ç†
                if outputs.is_cuda:
                    outputs = self.unified_memory_manager.migrate_to_device(
                        outputs, 'cpu', async_migration=False
                    )
                    self.stats['manual_migrations'] += 1
                    
            # åº”ç”¨ç½®ä¿¡åº¦é˜ˆå€¼
            mask = outputs[..., 4] > conf_threshold
            filtered_outputs = outputs[mask]
            
            # å¦‚æœéœ€è¦ï¼Œè¿ç§»å›GPU
            if access_pattern == 'gpu_heavy' and not filtered_outputs.is_cuda:
                filtered_outputs = self.unified_memory_manager.migrate_to_device(
                    filtered_outputs, self.device, async_migration=True
                )
                self.stats['auto_migrations'] += 1
                
            processing_time = time.time() - start_time
            self.stats['gpu_postprocessing_time'].append(processing_time)
            self.stats['total_processing_time'] += processing_time
            
            return filtered_outputs
            
        except Exception as e:
            print(f"[WARNING] ç»Ÿä¸€å†…å­˜åå¤„ç†å¤±è´¥: {e}")
            # å›é€€åˆ°ä¼ ç»Ÿæ–¹æ³•
            mask = outputs[..., 4] > conf_threshold
            return outputs[mask]
            
    def optimize_memory_access_patterns(self):
        """ä¼˜åŒ–å†…å­˜è®¿é—®æ¨¡å¼"""
        print("[INFO] ğŸ”„ å¼€å§‹ä¼˜åŒ–ç»Ÿä¸€å†…å­˜è®¿é—®æ¨¡å¼...")
        
        # è®©ç»Ÿä¸€å†…å­˜ç®¡ç†å™¨åˆ†æè®¿é—®æ¨¡å¼
        self.unified_memory_manager.optimize_access_patterns()
        
        # è®¡ç®—å†…å­˜æ•ˆç‡åˆ†æ•°
        total_hits = self.stats['unified_memory_hits']
        total_accesses = total_hits + self.stats['unified_memory_misses']
        
        if total_accesses > 0:
            hit_rate = total_hits / total_accesses
            migration_efficiency = 1.0 - (self.stats['auto_migrations'] / max(total_accesses, 1))
            self.stats['memory_efficiency_score'] = (hit_rate + migration_efficiency) / 2
            
        print(f"[INFO] ğŸ“Š å†…å­˜æ•ˆç‡åˆ†æ•°: {self.stats['memory_efficiency_score']:.3f}")
        
    def get_unified_memory_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿä¸€å†…å­˜ç»Ÿè®¡ä¿¡æ¯"""
        base_stats = self.unified_memory_manager.get_unified_memory_stats()
        
        # æ·»åŠ å¤„ç†å™¨ç‰¹å®šçš„ç»Ÿè®¡
        processor_stats = {
            'preprocessing_avg_time_ms': np.mean(self.stats['gpu_preprocessing_time']) * 1000 if self.stats['gpu_preprocessing_time'] else 0,
            'postprocessing_avg_time_ms': np.mean(self.stats['gpu_postprocessing_time']) * 1000 if self.stats['gpu_postprocessing_time'] else 0,
            'unified_memory_hit_rate': self.stats['unified_memory_hits'] / max(self.stats['unified_memory_hits'] + self.stats['unified_memory_misses'], 1),
            'auto_migration_count': self.stats['auto_migrations'],
            'manual_migration_count': self.stats['manual_migrations'],
            'memory_efficiency_score': self.stats['memory_efficiency_score'],
            'total_processing_time_s': self.stats['total_processing_time']
        }
        
        return {**base_stats, **processor_stats}
        
    def get_memory_usage(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        base_usage = self.unified_memory_manager.get_memory_usage()
        
        # æ·»åŠ å¤„ç†å™¨ç‰¹å®šçš„å†…å­˜ä¿¡æ¯
        processor_usage = {
            'unified_buffers_count': len(self.unified_buffers),
            'buffer_access_patterns': self.buffer_access_patterns,
        }
        
        return {**base_usage, **processor_usage}
        
    def cleanup(self):
        """æ¸…ç†æ‰€æœ‰èµ„æº"""
        print("[INFO] ğŸ§¹ å¼€å§‹æ¸…ç†ç»Ÿä¸€å†…å­˜GPUå¤„ç†å™¨...")
        
        try:
            # æ¸…ç†é¢„åˆ†é…çš„ç¼“å†²åŒº
            self.unified_buffers.clear()
            self.buffer_access_patterns.clear()
            
            # æ¸…ç†ç»Ÿä¸€å†…å­˜ç®¡ç†å™¨
            self.unified_memory_manager.cleanup()
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # æ¸…ç†GPUç¼“å­˜
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                torch.cuda.synchronize()
                
            print("[INFO] âœ… ç»Ÿä¸€å†…å­˜GPUå¤„ç†å™¨æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"[ERROR] æ¸…ç†ç»Ÿä¸€å†…å­˜GPUå¤„ç†å™¨æ—¶å‡ºé”™: {e}")

# å…¨å±€ç»Ÿä¸€å†…å­˜GPUå¤„ç†å™¨å®ä¾‹
_unified_gpu_processor = None

def get_unified_gpu_processor(device_id: int = 0, unified_memory_size_gb: float = 2.0) -> UnifiedMemoryGPUProcessor:
    """è·å–å…¨å±€ç»Ÿä¸€å†…å­˜GPUå¤„ç†å™¨å®ä¾‹"""
    global _unified_gpu_processor
    if _unified_gpu_processor is None:
        _unified_gpu_processor = UnifiedMemoryGPUProcessor(device_id, unified_memory_size_gb)
    return _unified_gpu_processor

def cleanup_unified_gpu_processor():
    """æ¸…ç†å…¨å±€ç»Ÿä¸€å†…å­˜GPUå¤„ç†å™¨"""
    global _unified_gpu_processor
    if _unified_gpu_processor is not None:
        _unified_gpu_processor.cleanup()
        _unified_gpu_processor = None

if __name__ == "__main__":
    # æµ‹è¯•ç»Ÿä¸€å†…å­˜GPUå¤„ç†å™¨
    print("[INFO] ğŸ§ª å¼€å§‹ç»Ÿä¸€å†…å­˜GPUå¤„ç†å™¨æµ‹è¯•...")
    
    processor = UnifiedMemoryGPUProcessor(device_id=0, unified_memory_size_gb=1.0)
    
    # æµ‹è¯•å›¾åƒé¢„å¤„ç†
    print("\n[TEST] æµ‹è¯•ç»Ÿä¸€å†…å­˜å›¾åƒé¢„å¤„ç†...")
    test_img = np.random.randint(0, 255, (1080, 1920, 3), dtype=np.uint8)
    
    # æµ‹è¯•ä¸åŒè®¿é—®æ¨¡å¼
    access_patterns = ['cpu_heavy', 'gpu_heavy', 'mixed']
    
    for pattern in access_patterns:
        print(f"\n[TEST] æµ‹è¯•è®¿é—®æ¨¡å¼: {pattern}")
        start_time = time.time()
        processed = processor.preprocess_image_unified(test_img, access_pattern=pattern)
        processing_time = time.time() - start_time
        
        print(f"  å¤„ç†æ—¶é—´: {processing_time*1000:.2f}ms")
        print(f"  è¾“å‡ºå½¢çŠ¶: {processed.shape}")
        print(f"  è¾“å‡ºè®¾å¤‡: {processed.device}")
        
    # æµ‹è¯•åå¤„ç†
    print("\n[TEST] æµ‹è¯•ç»Ÿä¸€å†…å­˜åå¤„ç†...")
    dummy_outputs = torch.rand(100, 6, device=processor.device)
    dummy_outputs[:, 4] = torch.rand(100) * 0.8 + 0.2  # ç½®ä¿¡åº¦
    
    filtered = processor.postprocess_detections_unified(dummy_outputs, conf_threshold=0.5)
    print(f"  è¿‡æ»¤å‰: {dummy_outputs.shape[0]} ä¸ªæ£€æµ‹")
    print(f"  è¿‡æ»¤å: {filtered.shape[0]} ä¸ªæ£€æµ‹")
    
    # ä¼˜åŒ–è®¿é—®æ¨¡å¼
    print("\n[TEST] æµ‹è¯•è®¿é—®æ¨¡å¼ä¼˜åŒ–...")
    processor.optimize_memory_access_patterns()
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\n[INFO] ğŸ“Š ç»Ÿä¸€å†…å­˜ç»Ÿè®¡:")
    stats = processor.get_unified_memory_stats()
    for key, value in stats.items():
        if isinstance(value, float):
            print(f"  {key}: {value:.3f}")
        else:
            print(f"  {key}: {value}")
    
    print("\n[INFO] ğŸ“ˆ å†…å­˜ä½¿ç”¨æƒ…å†µ:")
    usage = processor.get_memory_usage()
    for device, info in usage.items():
        if isinstance(info, dict):
            print(f"  {device}:")
            for k, v in info.items():
                print(f"    {k}: {v}")
        else:
            print(f"  {device}: {info}")
    
    # æ¸…ç†
    processor.cleanup()
    print("\n[INFO] âœ… æµ‹è¯•å®Œæˆ")