#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUå†…å­˜æœ€å¤§åŒ–é…ç½®å™¨
é’ˆå¯¹RTX 4060 8GBæ˜¾å­˜ï¼Œæœ€å¤§åŒ–GPUé‡åº¦è®¿é—®æ¨¡å¼çš„å†…å­˜åˆ©ç”¨
"""

import json
import os
import torch
import GPUtil
from typing import Dict, Any

class GPUMemoryMaximizer:
    """GPUå†…å­˜æœ€å¤§åŒ–é…ç½®å™¨"""
    
    def __init__(self):
        self.config_file = "gui_config.json"
        
    def analyze_gpu_memory_capacity(self) -> Dict[str, Any]:
        """åˆ†æGPUå†…å­˜å®¹é‡å’Œä½¿ç”¨æƒ…å†µ"""
        analysis = {
            'gpu_available': torch.cuda.is_available(),
            'total_vram_gb': 0,
            'available_vram_gb': 0,
            'current_usage_gb': 0,
            'recommended_unified_memory_gb': 0,
            'max_safe_allocation_gb': 0
        }
        
        if torch.cuda.is_available():
            try:
                # è·å–GPUä¿¡æ¯
                gpus = GPUtil.getGPUs()
                if gpus:
                    gpu = gpus[0]  # ä¸»GPU
                    analysis['total_vram_gb'] = gpu.memoryTotal / 1024  # MBè½¬GB
                    analysis['current_usage_gb'] = gpu.memoryUsed / 1024
                    analysis['available_vram_gb'] = (gpu.memoryTotal - gpu.memoryUsed) / 1024
                    
                    # è®¡ç®—æ¨èçš„ç»Ÿä¸€å†…å­˜å¤§å°
                    total_vram = analysis['total_vram_gb']
                    
                    if total_vram >= 8:  # 8GB+æ˜¾å­˜
                        # ä¸ºGPUé‡åº¦æ¨¡å¼é¢„ç•™æ›´å¤šå†…å­˜
                        # ä¿ç•™1.5GBç»™ç³»ç»Ÿå’Œå…¶ä»–è¿›ç¨‹ï¼Œå…¶ä½™ç”¨äºç»Ÿä¸€å†…å­˜
                        analysis['recommended_unified_memory_gb'] = total_vram - 1.5
                        analysis['max_safe_allocation_gb'] = total_vram - 1.0  # æœ€æ¿€è¿›é…ç½®
                    elif total_vram >= 6:  # 6GBæ˜¾å­˜
                        analysis['recommended_unified_memory_gb'] = total_vram - 1.0
                        analysis['max_safe_allocation_gb'] = total_vram - 0.5
                    else:  # 4GBåŠä»¥ä¸‹
                        analysis['recommended_unified_memory_gb'] = total_vram - 0.5
                        analysis['max_safe_allocation_gb'] = total_vram - 0.3
                        
            except Exception as e:
                print(f"[ERROR] GPUä¿¡æ¯è·å–å¤±è´¥: {e}")
        
        return analysis
    
    def calculate_optimal_gpu_config(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """è®¡ç®—æœ€ä¼˜GPUé…ç½®"""
        config = {
            "use_unified_memory": True,
            "unified_memory_access_pattern": "gpu_heavy",
            "enable_auto_migration": True,
            "fallback_to_traditional_gpu": True,
            "performance_monitoring": True,
            "memory_pool_preallocation": True,
            "zero_copy_optimization": True,
            "debug_unified_memory": False
        }
        
        # æ ¹æ®æ˜¾å­˜å®¹é‡è®¾ç½®ç»Ÿä¸€å†…å­˜å¤§å°
        total_vram = analysis['total_vram_gb']
        
        if total_vram >= 8:  # RTX 4060ç­‰8GBæ˜¾å­˜
            config.update({
                "unified_memory_size_gb": 6.5,  # å¤§å¹…æå‡åˆ°6.5GB
                "memory_optimization_level": "maximum",
                "gpu_memory_pool_size_mb": 1024,  # 1GBå†…å­˜æ± 
                "enable_large_batch_processing": True,
                "gpu_cache_size_mb": 512,
                "enable_memory_compression": True,
                "aggressive_gpu_allocation": True
            })
        elif total_vram >= 6:  # 6GBæ˜¾å­˜
            config.update({
                "unified_memory_size_gb": 5.0,
                "memory_optimization_level": "aggressive",
                "gpu_memory_pool_size_mb": 768,
                "enable_large_batch_processing": True,
                "gpu_cache_size_mb": 384
            })
        else:  # 4GBåŠä»¥ä¸‹
            config.update({
                "unified_memory_size_gb": 3.5,
                "memory_optimization_level": "balanced",
                "gpu_memory_pool_size_mb": 512,
                "gpu_cache_size_mb": 256
            })
        
        # GPUé‡åº¦è®¿é—®æ¨¡å¼ä¸“ç”¨ä¼˜åŒ–
        config.update({
            # å†…å­˜ç®¡ç†ä¼˜åŒ–
            "enable_gpu_preprocessing": True,
            "enable_gpu_postprocessing": True,
            "gpu_batch_processing": True,
            "gpu_stream_processing": True,
            "async_gpu_processing": True,
            
            # æ•°æ®ä¼ è¾“ä¼˜åŒ–
            "gpu_pipeline_optimization": True,
            "enable_memory_prefetch": True,
            "gpu_cache_optimization": True,
            "zero_copy_data_transfer": True,
            
            # ç³»ç»Ÿå†…å­˜å¸è½½
            "offload_to_gpu": True,
            "reduce_cpu_memory_usage": True,
            "enable_gpu_image_processing": True,
            "gpu_screen_capture": True,
            "gpu_image_resize": True,
            "gpu_color_conversion": True,
            
            # é«˜çº§ä¼˜åŒ–
            "enable_tensor_cores": True,
            "mixed_precision_processing": True,
            "gpu_memory_defragmentation": True,
            "dynamic_memory_allocation": True,
            
            # æ‰¹å¤„ç†ä¼˜åŒ–
            "increase_batch_size": True,
            "adaptive_batch_sizing": True,
            "batch_size_multiplier": 2.0,
            
            # ç¼“å­˜ç­–ç•¥
            "enable_persistent_cache": True,
            "cache_preloading": True,
            "intelligent_cache_eviction": True
        })
        
        return config
    
    def update_config_with_maximized_memory(self, optimal_config: Dict[str, Any]) -> bool:
        """æ›´æ–°é…ç½®æ–‡ä»¶ä¸ºæœ€å¤§åŒ–å†…å­˜é…ç½®"""
        try:
            # è¯»å–ç°æœ‰é…ç½®
            current_config = {}
            if os.path.exists(self.config_file):
                with open(self.config_file, "r", encoding="utf-8") as f:
                    current_config = json.load(f)
            
            # æ›´æ–°ç»Ÿä¸€å†…å­˜é…ç½®
            current_config["unified_memory"] = optimal_config
            
            # ä¿å­˜é…ç½®
            with open(self.config_file, "w", encoding="utf-8") as f:
                json.dump(current_config, f, indent=2, ensure_ascii=False)
            
            print(f"[SUCCESS] âœ… GPUå†…å­˜æœ€å¤§åŒ–é…ç½®å·²æ›´æ–°")
            return True
            
        except Exception as e:
            print(f"[ERROR] âŒ é…ç½®æ›´æ–°å¤±è´¥: {e}")
            return False
    
    def print_memory_maximization_report(self, analysis: Dict[str, Any], config: Dict[str, Any]):
        """æ‰“å°å†…å­˜æœ€å¤§åŒ–æŠ¥å‘Š"""
        print("\n" + "="*70)
        print("ğŸš€ GPUå†…å­˜æœ€å¤§åŒ–é…ç½®æŠ¥å‘Š")
        print("="*70)
        
        # GPUç¡¬ä»¶ä¿¡æ¯
        print(f"\nğŸ’¾ GPUç¡¬ä»¶ä¿¡æ¯:")
        print(f"  â€¢ æ€»æ˜¾å­˜: {analysis['total_vram_gb']:.1f}GB")
        print(f"  â€¢ å½“å‰ä½¿ç”¨: {analysis['current_usage_gb']:.1f}GB")
        print(f"  â€¢ å¯ç”¨æ˜¾å­˜: {analysis['available_vram_gb']:.1f}GB")
        
        # å†…å­˜åˆ†é…ç­–ç•¥
        print(f"\nâš™ï¸ å†…å­˜åˆ†é…ç­–ç•¥:")
        old_size = 2.5  # ä¹‹å‰çš„é…ç½®
        new_size = config.get('unified_memory_size_gb', 0)
        increase_percent = ((new_size - old_size) / old_size) * 100
        
        print(f"  â€¢ ç»Ÿä¸€å†…å­˜: {old_size}GB â†’ {new_size}GB (+{increase_percent:.0f}%)")
        print(f"  â€¢ å†…å­˜æ± : {config.get('gpu_memory_pool_size_mb', 0)}MB")
        print(f"  â€¢ GPUç¼“å­˜: {config.get('gpu_cache_size_mb', 0)}MB")
        print(f"  â€¢ ä¼˜åŒ–çº§åˆ«: {config.get('memory_optimization_level', 'N/A')}")
        
        # å¯ç”¨çš„ä¼˜åŒ–åŠŸèƒ½
        print(f"\nğŸ”§ å¯ç”¨çš„ä¼˜åŒ–åŠŸèƒ½:")
        optimizations = [
            ("GPUé¢„å¤„ç†", config.get('enable_gpu_preprocessing')),
            ("GPUåå¤„ç†", config.get('enable_gpu_postprocessing')),
            ("æ‰¹å¤„ç†ä¼˜åŒ–", config.get('gpu_batch_processing')),
            ("æµå¤„ç†", config.get('gpu_stream_processing')),
            ("å¼‚æ­¥å¤„ç†", config.get('async_gpu_processing')),
            ("ç®¡é“ä¼˜åŒ–", config.get('gpu_pipeline_optimization')),
            ("å†…å­˜é¢„å–", config.get('enable_memory_prefetch')),
            ("é›¶æ‹·è´ä¼ è¾“", config.get('zero_copy_data_transfer')),
            ("Tensor Core", config.get('enable_tensor_cores')),
            ("æ··åˆç²¾åº¦", config.get('mixed_precision_processing')),
            ("åŠ¨æ€å†…å­˜åˆ†é…", config.get('dynamic_memory_allocation')),
            ("æŒä¹…åŒ–ç¼“å­˜", config.get('enable_persistent_cache'))
        ]
        
        for name, enabled in optimizations:
            status = "âœ…" if enabled else "âŒ"
            print(f"  â€¢ {name}: {status}")
        
        # å†…å­˜ä½¿ç”¨é¢„æµ‹
        print(f"\nğŸ“Š å†…å­˜ä½¿ç”¨é¢„æµ‹:")
        system_reserve = analysis['total_vram_gb'] - new_size
        utilization = (new_size / analysis['total_vram_gb']) * 100
        
        print(f"  â€¢ GPUé‡åº¦æ¨¡å¼ä½¿ç”¨: {new_size}GB ({utilization:.1f}%)")
        print(f"  â€¢ ç³»ç»Ÿé¢„ç•™: {system_reserve:.1f}GB")
        print(f"  â€¢ é¢„æœŸGPUä½¿ç”¨ç‡: 70-90%")
        print(f"  â€¢ é¢„æœŸç³»ç»Ÿå†…å­˜é‡Šæ”¾: 2-3GB")
        
        # æ€§èƒ½æå‡é¢„æœŸ
        print(f"\nğŸ¯ æ€§èƒ½æå‡é¢„æœŸ:")
        print(f"  â€¢ å¤„ç†é€Ÿåº¦: +{increase_percent//2:.0f}%")
        print(f"  â€¢ GPUåˆ©ç”¨ç‡: 21% â†’ 70-90%")
        print(f"  â€¢ ç³»ç»Ÿå†…å­˜: 94.5% â†’ 70-80%")
        print(f"  â€¢ å¸§ç‡æå‡: +50-80%")
        print(f"  â€¢ å»¶è¿Ÿé™ä½: -40-60%")
        
        # é£é™©æç¤º
        print(f"\nâš ï¸ æ³¨æ„äº‹é¡¹:")
        print(f"  â€¢ ç¡®ä¿GPUæ•£çƒ­è‰¯å¥½")
        print(f"  â€¢ ç›‘æ§æ˜¾å­˜ä½¿ç”¨æƒ…å†µ")
        print(f"  â€¢ å¦‚é‡æ˜¾å­˜ä¸è¶³ï¼Œç¨‹åºä¼šè‡ªåŠ¨å›é€€")
        print(f"  â€¢ å»ºè®®å…³é—­å…¶ä»–GPUå¯†é›†å‹ç¨‹åº")
        
        print("="*70)
    
    def maximize_gpu_memory(self) -> bool:
        """æ‰§è¡ŒGPUå†…å­˜æœ€å¤§åŒ–"""
        print("ğŸ¯ GPUå†…å­˜æœ€å¤§åŒ–é…ç½®å™¨å¯åŠ¨")
        print("="*60)
        
        # åˆ†æGPUå†…å­˜å®¹é‡
        print("\n[STEP 1] ğŸ” åˆ†æGPUå†…å­˜å®¹é‡...")
        analysis = self.analyze_gpu_memory_capacity()
        
        if not analysis['gpu_available']:
            print("[ERROR] âŒ æœªæ£€æµ‹åˆ°å¯ç”¨çš„CUDA GPU")
            return False
        
        print(f"âœ… æ£€æµ‹åˆ° {analysis['total_vram_gb']:.1f}GB æ˜¾å­˜")
        
        # è®¡ç®—æœ€ä¼˜é…ç½®
        print("\n[STEP 2] âš™ï¸ è®¡ç®—æœ€ä¼˜GPUå†…å­˜é…ç½®...")
        optimal_config = self.calculate_optimal_gpu_config(analysis)
        
        # æ›´æ–°é…ç½®
        print("\n[STEP 3] ğŸ’¾ åº”ç”¨æœ€å¤§åŒ–å†…å­˜é…ç½®...")
        success = self.update_config_with_maximized_memory(optimal_config)
        
        if success:
            # æ‰“å°æŠ¥å‘Š
            self.print_memory_maximization_report(analysis, optimal_config)
            print("\nğŸ‰ GPUå†…å­˜æœ€å¤§åŒ–é…ç½®å®Œæˆï¼")
            return True
        else:
            print("\nâŒ é…ç½®å¤±è´¥")
            return False

def main():
    """ä¸»å‡½æ•°"""
    maximizer = GPUMemoryMaximizer()
    
    try:
        success = maximizer.maximize_gpu_memory()
        if success:
            print("\nğŸ’¡ æç¤º: é‡å¯AIç„å‡†ç¨‹åºä»¥åº”ç”¨æ–°çš„å†…å­˜é…ç½®")
            print("ğŸ’¡ å»ºè®®: è¿è¡Œ 'python gpu_realtime_monitor.py' ç›‘æ§æ•ˆæœ")
        return success
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ç”¨æˆ·ä¸­æ–­æ“ä½œ")
        return False
    except Exception as e:
        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        return False

if __name__ == "__main__":
    main()