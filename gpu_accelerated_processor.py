#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPUåŠ é€Ÿå›¾åƒå¤„ç†å™¨
å°†CPUå¯†é›†å‹çš„å›¾åƒé¢„å¤„ç†å’Œåå¤„ç†æ“ä½œè¿ç§»åˆ°GPU
è§£å†³å†…å­˜ä¸è¶³é—®é¢˜ï¼Œå……åˆ†åˆ©ç”¨GPUèµ„æº
"""

import numpy as np
import cv2
import time
import gc
from typing import Tuple, Optional, Any
import torch
import torch.nn.functional as F

try:
    import cupy as cp
    CUPY_AVAILABLE = True
    print("[INFO] âœ… CuPyå¯ç”¨ï¼Œå¯ç”¨CUDAåŠ é€Ÿ")
except ImportError:
    CUPY_AVAILABLE = False
    print("[WARNING] CuPyä¸å¯ç”¨ï¼Œä½¿ç”¨PyTorch CUDA")

class GPUAcceleratedProcessor:
    """GPUåŠ é€Ÿå›¾åƒå¤„ç†å™¨"""
    
    def __init__(self, device_id: int = 0, enable_memory_pool: bool = True):
        """
        åˆå§‹åŒ–GPUåŠ é€Ÿå¤„ç†å™¨
        
        Args:
            device_id: GPUè®¾å¤‡ID
            enable_memory_pool: æ˜¯å¦å¯ç”¨GPUå†…å­˜æ± 
        """
        self.device_id = device_id
        self.device = f'cuda:{device_id}' if torch.cuda.is_available() else 'cpu'
        self.enable_memory_pool = enable_memory_pool
        
        # GPUå†…å­˜æ± 
        self.memory_pool = {}
        self.pool_usage = {}
        
        # é¢„åˆ†é…å¸¸ç”¨å°ºå¯¸çš„GPUå†…å­˜
        self._preallocate_memory()
        
        # æ€§èƒ½ç»Ÿè®¡
        self.stats = {
            'gpu_preprocessing_time': [],
            'gpu_postprocessing_time': [],
            'memory_transfers': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }
        
        print(f"[INFO] ğŸš€ GPUåŠ é€Ÿå¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"[INFO] è®¾å¤‡: {self.device}")
        print(f"[INFO] å†…å­˜æ± : {'å¯ç”¨' if enable_memory_pool else 'ç¦ç”¨'}")
        
    def _preallocate_memory(self):
        """é¢„åˆ†é…GPUå†…å­˜æ± """
        if not torch.cuda.is_available():
            return
            
        try:
            # é¢„åˆ†é…å¸¸ç”¨å°ºå¯¸çš„å†…å­˜
            common_sizes = [
                (320, 320, 3),    # AIæ£€æµ‹å›¾åƒ
                (640, 640, 3),    # é«˜åˆ†è¾¨ç‡æ£€æµ‹
                (1920, 1080, 3),  # å…¨å±æˆªå›¾
                (1, 3, 320, 320), # NCHWæ ¼å¼
                (1, 3, 640, 640), # é«˜åˆ†è¾¨ç‡NCHW
            ]
            
            for size in common_sizes:
                key = f"buffer_{size}"
                if self.enable_memory_pool:
                    buffer = torch.empty(size, dtype=torch.float16, device=self.device)
                    self.memory_pool[key] = buffer
                    self.pool_usage[key] = False
                    
            print(f"[INFO] ğŸ“¦ é¢„åˆ†é…äº†{len(common_sizes)}ä¸ªGPUå†…å­˜ç¼“å†²åŒº")
            
        except Exception as e:
            print(f"[WARNING] GPUå†…å­˜é¢„åˆ†é…å¤±è´¥: {e}")
            
    def get_gpu_buffer(self, shape: Tuple, dtype=torch.float16) -> torch.Tensor:
        """
        è·å–GPUå†…å­˜ç¼“å†²åŒºï¼ˆæ”¯æŒå†…å­˜æ± å¤ç”¨ï¼‰
        
        Args:
            shape: å¼ é‡å½¢çŠ¶
            dtype: æ•°æ®ç±»å‹
            
        Returns:
            GPUå¼ é‡ç¼“å†²åŒº
        """
        key = f"buffer_{shape}"
        
        # å°è¯•ä»å†…å­˜æ± è·å–
        if self.enable_memory_pool and key in self.memory_pool:
            if not self.pool_usage[key]:
                self.pool_usage[key] = True
                self.stats['cache_hits'] += 1
                return self.memory_pool[key]
        
        # åˆ›å»ºæ–°çš„ç¼“å†²åŒº
        self.stats['cache_misses'] += 1
        return torch.empty(shape, dtype=dtype, device=self.device)
        
    def release_gpu_buffer(self, shape: Tuple):
        """é‡Šæ”¾GPUå†…å­˜ç¼“å†²åŒº"""
        key = f"buffer_{shape}"
        if key in self.pool_usage:
            self.pool_usage[key] = False
            
    def preprocess_image_gpu(self, img: np.ndarray, target_size: Tuple[int, int] = (320, 320)) -> torch.Tensor:
        """
        GPUåŠ é€Ÿå›¾åƒé¢„å¤„ç†
        
        Args:
            img: è¾“å…¥å›¾åƒ (numpyæ•°ç»„)
            target_size: ç›®æ ‡å°ºå¯¸
            
        Returns:
            é¢„å¤„ç†åçš„GPUå¼ é‡ (NCHWæ ¼å¼)
        """
        start_time = time.time()
        
        try:
            if CUPY_AVAILABLE:
                # ä½¿ç”¨CuPyè¿›è¡ŒGPUåŠ é€Ÿ
                return self._preprocess_with_cupy(img, target_size)
            else:
                # ä½¿ç”¨PyTorch CUDAè¿›è¡ŒåŠ é€Ÿ
                return self._preprocess_with_torch(img, target_size)
                
        except Exception as e:
            print(f"[WARNING] GPUé¢„å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°CPU: {e}")
            return self._preprocess_cpu_fallback(img, target_size)
        finally:
            processing_time = time.time() - start_time
            self.stats['gpu_preprocessing_time'].append(processing_time)
            
    def _preprocess_with_cupy(self, img: np.ndarray, target_size: Tuple[int, int]) -> torch.Tensor:
        """ä½¿ç”¨CuPyè¿›è¡ŒGPUåŠ é€Ÿé¢„å¤„ç†"""
        # å°†numpyæ•°ç»„è½¬æ¢ä¸ºCuPyæ•°ç»„ï¼ˆGPUï¼‰
        gpu_img = cp.asarray(img)
        
        # ç§»é™¤alphaé€šé“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if gpu_img.shape[2] == 4:
            gpu_img = gpu_img[:, :, :3]
        
        # GPUä¸Šè¿›è¡Œå›¾åƒç¼©æ”¾
        if gpu_img.shape[:2] != target_size:
            # CuPyæ²¡æœ‰ç›´æ¥çš„resizeï¼Œä½¿ç”¨PyTorch
            torch_img = torch.from_numpy(cp.asnumpy(gpu_img)).to(self.device)
            torch_img = torch_img.permute(2, 0, 1).unsqueeze(0).float()  # HWC -> NCHW
            torch_img = F.interpolate(torch_img, size=target_size, mode='bilinear', align_corners=False)
        else:
            torch_img = torch.from_numpy(cp.asnumpy(gpu_img)).to(self.device)
            torch_img = torch_img.permute(2, 0, 1).unsqueeze(0).float()
        
        # å½’ä¸€åŒ–å’Œç±»å‹è½¬æ¢
        torch_img = torch_img / 255.0
        torch_img = torch_img.half()  # è½¬æ¢ä¸ºfloat16
        
        self.stats['memory_transfers'] += 1
        return torch_img
        
    def _preprocess_with_torch(self, img: np.ndarray, target_size: Tuple[int, int]) -> torch.Tensor:
        """ä½¿ç”¨PyTorch CUDAè¿›è¡ŒåŠ é€Ÿé¢„å¤„ç†"""
        # è½¬æ¢ä¸ºPyTorchå¼ é‡å¹¶ç§»åˆ°GPU
        torch_img = torch.from_numpy(img).to(self.device)
        
        # ç§»é™¤alphaé€šé“ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if torch_img.shape[2] == 4:
            torch_img = torch_img[:, :, :3]
            
        # è½¬æ¢ä¸ºNCHWæ ¼å¼
        torch_img = torch_img.permute(2, 0, 1).unsqueeze(0).float()
        
        # GPUä¸Šè¿›è¡Œå›¾åƒç¼©æ”¾
        if torch_img.shape[2:] != target_size:
            torch_img = F.interpolate(torch_img, size=target_size, mode='bilinear', align_corners=False)
        
        # å½’ä¸€åŒ–å’Œç±»å‹è½¬æ¢
        torch_img = torch_img / 255.0
        torch_img = torch_img.half()  # è½¬æ¢ä¸ºfloat16èŠ‚çœæ˜¾å­˜
        
        self.stats['memory_transfers'] += 1
        return torch_img
        
    def _preprocess_cpu_fallback(self, img: np.ndarray, target_size: Tuple[int, int]) -> torch.Tensor:
        """CPUå›é€€é¢„å¤„ç†"""
        # ä¼ ç»Ÿçš„CPUé¢„å¤„ç†
        if img.shape[:2] != target_size:
            img = cv2.resize(img, target_size)
            
        if img.shape[2] == 4:
            img = img[:, :, :3]
            
        img = img.astype(np.float16) / 255.0
        img = np.moveaxis(img, 2, 0)  # HWC -> CHW
        img = np.expand_dims(img, 0)  # CHW -> NCHW
        
        return torch.from_numpy(img).to(self.device)
        
    def postprocess_detections_gpu(self, outputs: torch.Tensor, conf_threshold: float = 0.5) -> torch.Tensor:
        """
        GPUåŠ é€Ÿåå¤„ç†
        
        Args:
            outputs: æ¨¡å‹è¾“å‡ºå¼ é‡
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            å¤„ç†åçš„æ£€æµ‹ç»“æœ
        """
        start_time = time.time()
        
        try:
            # ç¡®ä¿å¼ é‡åœ¨GPUä¸Š
            if not outputs.is_cuda:
                outputs = outputs.to(self.device)
            
            # GPUä¸Šè¿›è¡Œç½®ä¿¡åº¦ç­›é€‰
            conf_mask = outputs[..., 4] > conf_threshold
            filtered_outputs = outputs[conf_mask]
            
            # GPUä¸Šè¿›è¡Œåæ ‡è½¬æ¢ç­‰è®¡ç®—
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šGPUåŠ é€Ÿçš„åå¤„ç†æ“ä½œ
            
            return filtered_outputs
            
        except Exception as e:
            print(f"[WARNING] GPUåå¤„ç†å¤±è´¥: {e}")
            return outputs
        finally:
            processing_time = time.time() - start_time
            self.stats['gpu_postprocessing_time'].append(processing_time)
            
    def apply_mask_gpu(self, img: torch.Tensor, mask_config: dict) -> torch.Tensor:
        """GPUåŠ é€Ÿæ©ç åº”ç”¨"""
        if not mask_config.get('enabled', False):
            return img
            
        try:
            # åœ¨GPUä¸Šåº”ç”¨æ©ç 
            mask_side = mask_config.get('side', 'right').lower()
            mask_width = mask_config.get('width', 100)
            mask_height = mask_config.get('height', 100)
            
            if mask_side == 'right':
                img[:, :, -mask_height:, -mask_width:] = 0
            elif mask_side == 'left':
                img[:, :, -mask_height:, :mask_width] = 0
                
            return img
            
        except Exception as e:
            print(f"[WARNING] GPUæ©ç åº”ç”¨å¤±è´¥: {e}")
            return img
            
    def get_memory_usage(self) -> dict:
        """è·å–GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
        if not torch.cuda.is_available():
            return {'gpu_memory_used': 0, 'gpu_memory_total': 0}
            
        try:
            memory_used = torch.cuda.memory_allocated(self.device_id) / 1024**3  # GB
            memory_total = torch.cuda.get_device_properties(self.device_id).total_memory / 1024**3  # GB
            
            return {
                'gpu_memory_used': memory_used,
                'gpu_memory_total': memory_total,
                'gpu_memory_percent': (memory_used / memory_total) * 100
            }
        except:
            return {'gpu_memory_used': 0, 'gpu_memory_total': 0}
            
    def get_performance_stats(self) -> dict:
        """è·å–æ€§èƒ½ç»Ÿè®¡"""
        stats = self.stats.copy()
        
        # è®¡ç®—å¹³å‡æ—¶é—´
        if stats['gpu_preprocessing_time']:
            stats['avg_preprocessing_time'] = np.mean(stats['gpu_preprocessing_time'])
            stats['max_preprocessing_time'] = np.max(stats['gpu_preprocessing_time'])
            
        if stats['gpu_postprocessing_time']:
            stats['avg_postprocessing_time'] = np.mean(stats['gpu_postprocessing_time'])
            stats['max_postprocessing_time'] = np.max(stats['gpu_postprocessing_time'])
            
        # ç¼“å­˜å‘½ä¸­ç‡
        total_requests = stats['cache_hits'] + stats['cache_misses']
        if total_requests > 0:
            stats['cache_hit_rate'] = stats['cache_hits'] / total_requests * 100
        else:
            stats['cache_hit_rate'] = 0
            
        return stats
        
    def cleanup(self):
        """æ¸…ç†GPUèµ„æº"""
        try:
            # æ¸…ç©ºå†…å­˜æ± 
            for key in self.memory_pool:
                del self.memory_pool[key]
            self.memory_pool.clear()
            self.pool_usage.clear()
            
            # å¼ºåˆ¶GPUåƒåœ¾å›æ”¶
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
                
            # æ¸…ç©ºç»Ÿè®¡ä¿¡æ¯
            for key in self.stats:
                if isinstance(self.stats[key], list):
                    self.stats[key].clear()
                else:
                    self.stats[key] = 0
                    
            print("[INFO] ğŸ§¹ GPUèµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"[WARNING] GPUèµ„æºæ¸…ç†å¤±è´¥: {e}")

# å…¨å±€GPUå¤„ç†å™¨å®ä¾‹
_gpu_processor = None

def get_gpu_processor(device_id: int = 0) -> GPUAcceleratedProcessor:
    """è·å–GPUå¤„ç†å™¨å•ä¾‹"""
    global _gpu_processor
    if _gpu_processor is None:
        _gpu_processor = GPUAcceleratedProcessor(device_id)
    return _gpu_processor

def cleanup_gpu_processor():
    """æ¸…ç†GPUå¤„ç†å™¨"""
    global _gpu_processor
    if _gpu_processor is not None:
        _gpu_processor.cleanup()
        _gpu_processor = None

if __name__ == "__main__":
    # æµ‹è¯•GPUåŠ é€Ÿå¤„ç†å™¨
    processor = GPUAcceleratedProcessor()
    
    # åˆ›å»ºæµ‹è¯•å›¾åƒ
    test_img = np.random.randint(0, 255, (1080, 1920, 3), dtype=np.uint8)
    
    print("[INFO] ğŸ§ª å¼€å§‹GPUåŠ é€Ÿæµ‹è¯•...")
    
    # æµ‹è¯•é¢„å¤„ç†
    start_time = time.time()
    processed = processor.preprocess_image_gpu(test_img)
    gpu_time = time.time() - start_time
    
    print(f"[INFO] GPUé¢„å¤„ç†æ—¶é—´: {gpu_time*1000:.2f}ms")
    print(f"[INFO] è¾“å‡ºå½¢çŠ¶: {processed.shape}")
    print(f"[INFO] è¾“å‡ºè®¾å¤‡: {processed.device}")
    
    # è·å–æ€§èƒ½ç»Ÿè®¡
    stats = processor.get_performance_stats()
    print(f"[INFO] ç¼“å­˜å‘½ä¸­ç‡: {stats['cache_hit_rate']:.1f}%")
    
    # è·å–å†…å­˜ä½¿ç”¨
    memory = processor.get_memory_usage()
    print(f"[INFO] GPUå†…å­˜ä½¿ç”¨: {memory['gpu_memory_used']:.2f}GB / {memory['gpu_memory_total']:.2f}GB")
    
    # æ¸…ç†èµ„æº
    processor.cleanup()
    print("[INFO] âœ… æµ‹è¯•å®Œæˆ")