#!/usr/bin/env python3
"""
æ¨¡å‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•è„šæœ¬
æ¯”è¾ƒ yolov5s320Half.onnx å’Œ yolov5m320Half.onnx çš„æ€§èƒ½å·®å¼‚
"""

import time
import numpy as np
import torch
import torch.nn.functional as F
import onnxruntime as ort
import psutil
import GPUtil
from typing import Dict, List, Tuple

class ModelPerformanceComparator:
    def __init__(self):
        self.models = {
            'yolov5s': 'yolov5s320Half.onnx',
            'yolov5m': 'yolov5m320Half.onnx'
        }
        self.test_results = {}
        
    def create_test_input(self, use_fp16: bool = True) -> np.ndarray:
        """åˆ›å»ºæµ‹è¯•è¾“å…¥æ•°æ® (320x320x3)"""
        data = np.random.rand(1, 3, 320, 320)
        if use_fp16:
            return data.astype(np.float16)
        else:
            return data.astype(np.float32)
    
    def create_onnx_session(self, model_path: str) -> ort.InferenceSession:
        """åˆ›å»ºä¼˜åŒ–çš„ONNXæ¨ç†ä¼šè¯"""
        try:
            # ä¼šè¯é€‰é¡¹
            so = ort.SessionOptions()
            so.log_severity_level = 3  # å‡å°‘æ—¥å¿—è¾“å‡º
            
            # æä¾›è€…é…ç½®
            providers = [
                ('CUDAExecutionProvider', {
                    'device_id': 0,
                    'arena_extend_strategy': 'kNextPowerOfTwo',
                    'gpu_mem_limit': 4 * 1024 * 1024 * 1024,  # 4GB
                    'cudnn_conv_algo_search': 'EXHAUSTIVE',
                    'do_copy_in_default_stream': True,
                }),
                'CPUExecutionProvider'
            ]
            
            session = ort.InferenceSession(model_path, sess_options=so, providers=providers)
            print(f"âœ… {model_path} åŠ è½½æˆåŠŸ")
            return session
            
        except Exception as e:
            print(f"âŒ {model_path} åŠ è½½å¤±è´¥: {e}")
            return None
    
    def get_system_info(self) -> Dict:
        """è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ"""
        # CPUå’Œå†…å­˜ä¿¡æ¯
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        
        # GPUä¿¡æ¯
        gpu_info = {}
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                gpu_info = {
                    'name': gpu.name,
                    'memory_total': gpu.memoryTotal,
                    'memory_used': gpu.memoryUsed,
                    'memory_free': gpu.memoryFree,
                    'load': gpu.load * 100,
                    'temperature': gpu.temperature
                }
        except:
            gpu_info = {'error': 'GPUä¿¡æ¯è·å–å¤±è´¥'}
        
        return {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'memory_used_mb': memory.used / 1024 / 1024,
            'gpu': gpu_info
        }
    
    def benchmark_model(self, model_name: str, model_path: str, num_runs: int = 100) -> Dict:
        """å¯¹å•ä¸ªæ¨¡å‹è¿›è¡Œæ€§èƒ½æµ‹è¯•"""
        print(f"\nğŸ”„ å¼€å§‹æµ‹è¯• {model_name} ({model_path})")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        import os
        if not os.path.exists(model_path):
            return {'error': f'æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}'}
        
        # åˆ›å»ºä¼šè¯
        session = self.create_onnx_session(model_path)
        if session is None:
            return {'error': f'æ— æ³•åˆ›å»ºæ¨ç†ä¼šè¯: {model_path}'}
        
        # è·å–è¾“å…¥è¾“å‡ºä¿¡æ¯
        input_name = session.get_inputs()[0].name
        output_names = [output.name for output in session.get_outputs()]
        
        # åˆ›å»ºæµ‹è¯•æ•°æ® (åŠç²¾åº¦æ¨¡å‹éœ€è¦FP16è¾“å…¥)
        test_input = self.create_test_input(use_fp16=True)
        
        # é¢„çƒ­
        print("ğŸ”¥ é¢„çƒ­æ¨¡å‹...")
        for _ in range(10):
            _ = session.run(output_names, {input_name: test_input})
        
        # è·å–æµ‹è¯•å‰çš„ç³»ç»ŸçŠ¶æ€
        before_info = self.get_system_info()
        
        # æ€§èƒ½æµ‹è¯•
        print(f"â±ï¸ å¼€å§‹ {num_runs} æ¬¡æ¨ç†æµ‹è¯•...")
        inference_times = []
        
        start_time = time.time()
        for i in range(num_runs):
            inference_start = time.perf_counter()
            outputs = session.run(output_names, {input_name: test_input})
            inference_end = time.perf_counter()
            
            inference_times.append((inference_end - inference_start) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’
            
            if (i + 1) % 20 == 0:
                print(f"  å®Œæˆ {i + 1}/{num_runs} æ¬¡æ¨ç†")
        
        total_time = time.time() - start_time
        
        # è·å–æµ‹è¯•åçš„ç³»ç»ŸçŠ¶æ€
        after_info = self.get_system_info()
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        inference_times = torch.tensor(inference_times, device='cuda').cpu().numpy()
        
        results = {
            'model_name': model_name,
            'model_path': model_path,
            'num_runs': num_runs,
            'total_time_sec': total_time,
            'avg_inference_ms': np.mean(inference_times),
            'min_inference_ms': np.min(inference_times),
            'max_inference_ms': np.max(inference_times),
            'std_inference_ms': np.std(inference_times),
            'fps': 1000 / np.mean(inference_times),
            'system_before': before_info,
            'system_after': after_info,
            'memory_increase_mb': after_info['memory_used_mb'] - before_info['memory_used_mb']
        }
        
        if 'gpu' in after_info and 'gpu' in before_info:
            if 'memory_used' in after_info['gpu'] and 'memory_used' in before_info['gpu']:
                results['gpu_memory_increase_mb'] = after_info['gpu']['memory_used'] - before_info['gpu']['memory_used']
        
        return results
    
    def run_comparison(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„æ¨¡å‹å¯¹æ¯”æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹æ¨¡å‹æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
        print("=" * 60)
        
        results = {}
        
        for model_name, model_path in self.models.items():
            try:
                results[model_name] = self.benchmark_model(model_name, model_path)
            except Exception as e:
                results[model_name] = {'error': str(e)}
        
        return results
    
    def print_comparison_report(self, results: Dict):
        """æ‰“å°å¯¹æ¯”æŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("ğŸ“Š æ¨¡å‹æ€§èƒ½å¯¹æ¯”æŠ¥å‘Š")
        print("=" * 80)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆç»“æœ
        valid_results = {k: v for k, v in results.items() if 'error' not in v}
        
        if len(valid_results) < 2:
            print("âŒ æ— æ³•è¿›è¡Œå¯¹æ¯”ï¼Œç¼ºå°‘æœ‰æ•ˆçš„æµ‹è¯•ç»“æœ")
            for model_name, result in results.items():
                if 'error' in result:
                    print(f"  {model_name}: {result['error']}")
            return
        
        # åŸºæœ¬æ€§èƒ½å¯¹æ¯”
        print("\nğŸ¯ æ¨ç†æ€§èƒ½å¯¹æ¯”:")
        print(f"{'æ¨¡å‹':<15} {'å¹³å‡å»¶è¿Ÿ(ms)':<15} {'FPS':<10} {'æœ€å°å»¶è¿Ÿ(ms)':<15} {'æœ€å¤§å»¶è¿Ÿ(ms)':<15}")
        print("-" * 80)
        
        for model_name, result in valid_results.items():
            print(f"{model_name:<15} {result['avg_inference_ms']:<15.2f} {result['fps']:<10.1f} "
                  f"{result['min_inference_ms']:<15.2f} {result['max_inference_ms']:<15.2f}")
        
        # å†…å­˜ä½¿ç”¨å¯¹æ¯”
        print("\nğŸ’¾ å†…å­˜ä½¿ç”¨å¯¹æ¯”:")
        print(f"{'æ¨¡å‹':<15} {'ç³»ç»Ÿå†…å­˜å¢åŠ (MB)':<20} {'GPUå†…å­˜å¢åŠ (MB)':<20}")
        print("-" * 60)
        
        for model_name, result in valid_results.items():
            gpu_mem = result.get('gpu_memory_increase_mb', 'N/A')
            print(f"{model_name:<15} {result['memory_increase_mb']:<20.1f} {gpu_mem}")
        
        # æ€§èƒ½å·®å¼‚åˆ†æ
        if 'yolov5s' in valid_results and 'yolov5m' in valid_results:
            s_result = valid_results['yolov5s']
            m_result = valid_results['yolov5m']
            
            print("\nğŸ“ˆ æ€§èƒ½å·®å¼‚åˆ†æ:")
            
            # é€Ÿåº¦å·®å¼‚
            speed_diff = ((m_result['avg_inference_ms'] - s_result['avg_inference_ms']) / s_result['avg_inference_ms']) * 100
            fps_diff = ((s_result['fps'] - m_result['fps']) / m_result['fps']) * 100
            
            print(f"  æ¨ç†å»¶è¿Ÿå·®å¼‚: yolov5m æ¯” yolov5s æ…¢ {speed_diff:.1f}%")
            print(f"  FPSå·®å¼‚: yolov5s æ¯” yolov5m å¿« {fps_diff:.1f}%")
            
            # å†…å­˜å·®å¼‚
            mem_diff = m_result['memory_increase_mb'] - s_result['memory_increase_mb']
            print(f"  ç³»ç»Ÿå†…å­˜å·®å¼‚: yolov5m å¤šä½¿ç”¨ {mem_diff:.1f} MB")
            
            if 'gpu_memory_increase_mb' in m_result and 'gpu_memory_increase_mb' in s_result:
                gpu_mem_diff = m_result['gpu_memory_increase_mb'] - s_result['gpu_memory_increase_mb']
                print(f"  GPUå†…å­˜å·®å¼‚: yolov5m å¤šä½¿ç”¨ {gpu_mem_diff:.1f} MB")
        
        # å»ºè®®
        print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
        if 'yolov5s' in valid_results and 'yolov5m' in valid_results:
            s_fps = valid_results['yolov5s']['fps']
            m_fps = valid_results['yolov5m']['fps']
            
            if s_fps > 60 and m_fps > 30:
                print("  âœ… ä¸¤ä¸ªæ¨¡å‹çš„FPSéƒ½è¶³å¤Ÿé«˜ï¼Œå¯ä»¥è€ƒè™‘ä½¿ç”¨yolov5mè·å¾—æ›´å¥½çš„ç²¾åº¦")
            elif s_fps > 60 and m_fps < 30:
                print("  âš ï¸ yolov5mçš„FPSè¾ƒä½ï¼Œå»ºè®®ç»§ç»­ä½¿ç”¨yolov5sä¿è¯å®æ—¶æ€§")
            else:
                print("  âŒ ä¸¤ä¸ªæ¨¡å‹çš„FPSéƒ½ä¸å¤Ÿç†æƒ³ï¼Œå»ºè®®æ£€æŸ¥ç³»ç»Ÿé…ç½®")
        
        print("\n" + "=" * 80)

def main():
    """ä¸»å‡½æ•°"""
    comparator = ModelPerformanceComparator()
    
    # è¿è¡Œå¯¹æ¯”æµ‹è¯•
    results = comparator.run_comparison()
    
    # æ‰“å°æŠ¥å‘Š
    comparator.print_comparison_report(results)
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    import json
    with open('model_performance_comparison.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: model_performance_comparison.json")

if __name__ == "__main__":
    main()